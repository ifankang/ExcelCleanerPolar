{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTuV7736F7EG2oa90PblDF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifankang/ExcelCleanerPolar/blob/main/excel_cleaning_polar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Clean Excel Files\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def find_header_row(df_preview, min_non_nulls=3):\n",
        "    for idx, row in df_preview.iterrows():\n",
        "        if row.notna().sum() >= min_non_nulls:\n",
        "            return idx\n",
        "    return None\n",
        "\n",
        "def _clean_column_names(columns):\n",
        "    cleaned = pd.Series(columns).astype(str).str.strip().str.lower()\n",
        "    cleaned = cleaned.str.replace(r\"[^\\w]+\", \"_\", regex=True).str.strip(\"_\")\n",
        "    cleaned = cleaned.str.replace(r\"_{2,}\", \"_\", regex=True)\n",
        "\n",
        "    for dup in cleaned[cleaned.duplicated()].unique():\n",
        "        idxs = cleaned[cleaned == dup].index\n",
        "        cleaned.loc[idxs] = [f\"{dup}_{i}\" if i else dup for i in range(len(idxs))]\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def cleaning_df(df: pd.DataFrame) -> pd.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Cleans a pandas DataFrame that originated from an Excel file.\n",
        "\n",
        "    Args:\n",
        "        df: The input pandas DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A cleaned pandas DataFrame or None if cleaning fails.\n",
        "    \"\"\"\n",
        "    print(\"Starting DataFrame cleaning...\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Step 1: Find header row (using the find_header_row logic adapted for DataFrame)\n",
        "        # We can't use a file path anymore, so we need a way to detect the header row\n",
        "        # within the DataFrame itself. We'll look at the first few rows.\n",
        "        df_preview = df.head(20).copy() # Look at the first 20 rows for header detection\n",
        "\n",
        "        header_row_idx = None\n",
        "        # Assuming the header is a row where a reasonable number of columns are not null\n",
        "        # This is a heuristic and might need adjustment based on the specific data structure\n",
        "        for idx in range(len(df_preview)):\n",
        "             # Check if at least 3 non-null values exist in this row\n",
        "            if df_preview.iloc[idx].notna().sum() >= 3:\n",
        "                header_row_idx = idx\n",
        "                break\n",
        "\n",
        "\n",
        "        if header_row_idx is None:\n",
        "            print(f\"⚠️ Could not find valid header in the provided DataFrame.\")\n",
        "            return None\n",
        "\n",
        "        # # Step 2: Extract data from the detected header row onwards\n",
        "        # # The actual headers are in the detected header row (header_row_idx)\n",
        "        headers = df.iloc[header_row_idx].tolist()\n",
        "        # # The data starts from the row *after* the header row\n",
        "        df_data = df.iloc[header_row_idx+1:].copy()\n",
        "\n",
        "        df_data.columns = headers\n",
        "        df_data = df_data.dropna(how=\"all\", axis=1)     # Drop fully empty columns\n",
        "        # Drop mostly empty rows (rows with less than 50% non-null values)\n",
        "        df_data = df_data.dropna(thresh=int(df_data.shape[1] * 0.5))\n",
        "        df_data = df_data.ffill()                       # Forward-fill missing values\n",
        "\n",
        "\n",
        "        # Step 3: Clean column names to remove duplicates and unsafe characters\n",
        "        df_data.columns = _clean_column_names(df_data.columns)\n",
        "\n",
        "\n",
        "        print(\"✅ DataFrame cleaning complete.\")\n",
        "        return df_data.reset_index(drop=True)\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"❌ Error cleaning DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "# Ensure the cleaned_folder exists\n",
        "start_time = time.time()\n",
        "cleaned_folder = \"cleaned\"\n",
        "if not os.path.exists(cleaned_folder):\n",
        "    os.makedirs(cleaned_folder)\n",
        "\n",
        "# Iterate through files in the current directory\n",
        "for filename in os.listdir(\".\"):\n",
        "    if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):\n",
        "        print(f\"Processing {filename}...\")\n",
        "        try:\n",
        "            # Read the excel file\n",
        "            df = pd.read_excel(filename, engine='calamine')\n",
        "\n",
        "            # Clean the dataframe\n",
        "            cleaned_df = cleaning_df(df.copy())\n",
        "\n",
        "            if cleaned_df is not None:\n",
        "                # Define the output filename\n",
        "                output_filename = os.path.join(cleaned_folder, f\"cleaned_{filename}\")\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(f\"Time elapsed: {elapsed_time:.4f} seconds\")\n",
        "                start_time = time.time()\n",
        "                # Save the cleaned dataframe to a new excel file\n",
        "                cleaned_df.to_excel(output_filename, index=False, engine='xlsxwriter')\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(f\"Time exporting to excel: {elapsed_time:.4f} seconds\")\n",
        "                print(f\"Saved cleaned data to {output_filename}\")\n",
        "            else:\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(f\"Time elapsed: {elapsed_time:.4f} seconds\")\n",
        "                print(f\"Skipping saving for {filename} due to cleaning failure.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            print(f\"Time elapsed before error: {elapsed_time:.4f} seconds\")\n",
        "            print(f\"Error processing {filename}: {e}\")"
      ],
      "metadata": {
        "id": "yGdVUrg7fuQI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.head(20)"
      ],
      "metadata": {
        "id": "ik8Kq9eb0mYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkWgclMKaW0X"
      },
      "outputs": [],
      "source": [
        "# @title Clean Excel (Faster)\n",
        "%pip install -q fastexcel xlsxwriter polars\n",
        "import os\n",
        "import polars as pl\n",
        "import time\n",
        "import re\n",
        "import logging\n",
        "logging.getLogger('fastexcel.types.dtype').disabled = True\n",
        "\n",
        "def find_header_row_pl(df_preview: pl.DataFrame, min_non_nulls=3) -> int | None:\n",
        "    \"\"\"\n",
        "    Finds the index of the potential header row in a Polars DataFrame preview.\n",
        "    A row is considered a header row if it has at least `min_non_nulls` non-null values.\n",
        "    \"\"\"\n",
        "    # print(\"Debug: Finding header row...\")\n",
        "    for idx in range(df_preview.height):\n",
        "        row = df_preview.row(idx, named=False) # Get row as a tuple\n",
        "        # Count non-nulls in the tuple\n",
        "        non_null_count = sum(1 for x in row if x is not None and x != '') # Add check for empty strings\n",
        "        # print(f\"Debug: Row {idx}: Non-null count = {non_null_count}\")\n",
        "        if non_null_count >= min_non_nulls:\n",
        "            # print(f\"Debug: Found potential header row at index {idx}\")\n",
        "            return idx\n",
        "    # print(\"Debug: No valid header row found.\")\n",
        "    return None\n",
        "\n",
        "def _clean_column_names_pl(columns: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Cleans a list of column names for use in Polars DataFrame.\n",
        "    Replaces non-alphanumeric characters with underscores, removes leading/trailing underscores,\n",
        "    collapses multiple underscores, and handles duplicates by appending a number.\n",
        "    \"\"\"\n",
        "    # print(\"Debug: Cleaning column names...\")\n",
        "    cleaned = [col.strip().lower() if col is not None else \"\" for col in columns] # Handle None values\n",
        "    cleaned = [re.sub(r\"[^\\w]+\", \"_\", col) for col in cleaned]\n",
        "    cleaned = [col.strip(\"_\") for col in cleaned]\n",
        "    cleaned = [re.sub(r\"_{2,}\", \"_\", col) for col in cleaned]\n",
        "\n",
        "    counts = {}\n",
        "    cleaned_final = []\n",
        "    for col in cleaned:\n",
        "        if col in counts:\n",
        "            counts[col] += 1\n",
        "            cleaned_final.append(f\"{col}_{counts[col]}\")\n",
        "            print(f\"Debug: Renaming duplicate column '{col}' to '{col}_{counts[col]}'\")\n",
        "        else:\n",
        "            counts[col] = 0\n",
        "            cleaned_final.append(col)\n",
        "            # print(f\"Debug: Keeping column name '{col}'\")\n",
        "\n",
        "    # print(\"Debug: Column names cleaned.\")\n",
        "    return cleaned_final\n",
        "\n",
        "\n",
        "def cleaning_df_polars(df: pl.DataFrame) -> pl.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Cleans a Polars DataFrame that originated from an Excel file.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A cleaned Polars DataFrame or None if cleaning fails.\n",
        "    \"\"\"\n",
        "    print(\"Starting Polars DataFrame cleaning...\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Step 1: Find header row\n",
        "        df_preview = df.head(20) # Look at the first 20 rows for header detection\n",
        "        header_row_idx = find_header_row_pl(df_preview)\n",
        "\n",
        "        if header_row_idx is None:\n",
        "            print(f\"⚠️ Could not find valid header in the provided DataFrame.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "        # Step 2: Extract data from the detected header row onwards\n",
        "        # The actual headers are in the detected header row (header_row_idx)\n",
        "        headers = df.row(header_row_idx, named=False)\n",
        "        cleaned_columns = _clean_column_names_pl(headers)\n",
        "        # print(f\"Debug: Cleaned Headers: {cleaned_columns}\")\n",
        "\n",
        "\n",
        "        # The data starts from the row *after* the header row\n",
        "        # Slice the DataFrame first\n",
        "        df_data = df.slice(offset=header_row_idx + 1, length=df.height - (header_row_idx + 1))\n",
        "\n",
        "\n",
        "\n",
        "        # Now rename the sliced DataFrame using the cleaned headers\n",
        "        # Create a dictionary mapping the original column names of df_data to the cleaned column names\n",
        "        # The original column names are still the __UNNAMED ones from pl.read_excel\n",
        "        rename_dict = {old_col: new_col for old_col, new_col in zip(df_data.columns, cleaned_columns)}\n",
        "        df_data = df_data.rename(rename_dict)\n",
        "        # print(f\"Debug: Rename dictionary: {rename_dict}\")\n",
        "        # print(f\"Debug: DataFrame after renaming columns and slicing data. Shape: {df_data.shape}\")\n",
        "        # print(f\"Debug: Columns after renaming: {df_data.columns}\")\n",
        "\n",
        "        # Step 4: Clean data - Drop fully empty columns, drop mostly empty rows, ffill\n",
        "        # Drop fully empty columns (check if all values are null or empty string)\n",
        "        cols_to_drop = [col for col in df_data.columns if df_data[col].is_null().all() or (df_data[col] == '').all()]\n",
        "        if cols_to_drop:\n",
        "            # print(f\"Debug: Dropping fully empty columns: {cols_to_drop}\")\n",
        "            df_data = df_data.drop(cols_to_drop)\n",
        "            # print(f\"Debug: DataFrame shape after dropping empty columns: {df_data.shape}\")\n",
        "\n",
        "        # Drop mostly empty rows (rows with less than 25% non-null values)\n",
        "        # Calculate minimum non-null values required per row\n",
        "        min_non_null_per_row = int(df_data.shape[1] * 0.25)\n",
        "        print(f\"Debug: Dropping rows with less than {min_non_null_per_row} non-null values.\")\n",
        "\n",
        "        # Calculate row-wise non-null counts, excluding empty strings\n",
        "        # Use a list comprehension to create expressions for each column\n",
        "        non_null_exprs = [\n",
        "            pl.when(pl.col(c).is_not_null() & (pl.col(c).cast(pl.Utf8) != '')).then(1).otherwise(0)\n",
        "            for c in df_data.columns\n",
        "        ]\n",
        "\n",
        "        # Sum the non-null counts across columns for each row and filter\n",
        "        df_data = df_data.filter(\n",
        "            pl.sum_horizontal(non_null_exprs) >= min_non_null_per_row\n",
        "        )\n",
        "\n",
        "\n",
        "        # Remove completely empty rows\n",
        "        # df_data = df_data.filter(\n",
        "        #   pl.concat_list([pl.col(col).is_not_null() & (pl.col(col) != \"\") for col in df.columns])\n",
        "        # ).any(axis=1)\n",
        "\n",
        "        # Drop mostly empty rows (rows with less than 50% non-null values\n",
        "        # # Calculate minimum non-null values required per row\n",
        "        # min_non_null_per_row = int(df_data.shape[1] * 0.5)\n",
        "        # print(f\"Debug: Dropping rows with less than {min_non_null_per_row} non-null values.\")\n",
        "\n",
        "        # # Calculate row-wise non-null counts, excluding empty strings\n",
        "        # row_non_null_counts = df_data.select([\n",
        "        #     (pl.when(pl.col(c).is_not_null() & (pl.col(c).cast(pl.Utf8) != '')).then(1).otherwise(0)).sum().alias(c) for c in df_data.columns\n",
        "        # ]).sum().to_series()  # Convert to Series\n",
        "\n",
        "        # # Print non-null counts for debugging\n",
        "        # print(\"Debug: Non-null counts per row:\")\n",
        "        # print(row_non_null_counts)\n",
        "\n",
        "        # # Filter rows based on the calculated non-null counts\n",
        "        # df_data = df_data.filter(row_non_null_counts >= min_non_null_per_row)\n",
        "        # print(f\"Debug: DataFrame shape after dropping mostly empty rows: {df_data.shape}\")\n",
        "\n",
        "\n",
        "        # Forward-fill missing values\n",
        "        # Polars ffill works column-wise\n",
        "        df_data = df_data.fill_null(strategy=\"forward\")\n",
        "        # Also consider filling empty strings with the previous non-empty value if needed\n",
        "        # This is more complex and depends on the data type and desired behavior.\n",
        "        # For simplicity, fill nulls only.\n",
        "        return df_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error cleaning Polars DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "def polars_dataframe_dtype_validate(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Infers the data type for each column in a Polars DataFrame based on its content\n",
        "    and casts the column to the inferred type.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A Polars DataFrame with column types validated and casted based on content.\n",
        "    \"\"\"\n",
        "    print(\"Starting Polars DataFrame dtype validation...\")\n",
        "    new_df = df.clone() # Create a copy to avoid modifying the original DataFrame in place\n",
        "\n",
        "    for col in new_df.columns:\n",
        "        # print(f\"Debug: Processing column '{col}'...\")\n",
        "        # Skip if the column is entirely null\n",
        "        if new_df[col].is_null().all():\n",
        "            # print(f\"Debug: Column '{col}' is all null, keeping as Utf8.\")\n",
        "            continue # Keep the default Utf8 type for all-null columns\n",
        "\n",
        "        # Get the first non-null, non-empty string value to infer type\n",
        "        first_value = None\n",
        "        for value in new_df[col].to_list():\n",
        "            if value is not None and value != '' and (isinstance(value, str) and value.strip() != '' or not isinstance(value, str)):\n",
        "                 first_value = value\n",
        "                 break\n",
        "\n",
        "        inferred_type = pl.Utf8 # Default type\n",
        "\n",
        "        if first_value is not None:\n",
        "             # Try to infer a more specific type\n",
        "             if isinstance(first_value, (int, float)):\n",
        "                 inferred_type = pl.Float64\n",
        "             elif isinstance(first_value, str):\n",
        "                 try:\n",
        "                     # Attempt to parse as date\n",
        "                     # Use an explicit format string based on the observed data\n",
        "                     pl.from_epoch(new_df[col].str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\"), time_unit=\"ms\") # Check if can be parsed as Datetime first\n",
        "                     inferred_type = pl.Date # If successful, we can cast to Date\n",
        "                 except Exception: # Catch any exception during parsing\n",
        "                     try:\n",
        "                         # Attempt to parse as a number (float)\n",
        "                         float(first_value)\n",
        "                         inferred_type = pl.Float64\n",
        "                     except (ValueError, TypeError):\n",
        "                          # If neither date nor number, keep as string\n",
        "                         inferred_type = pl.Utf8\n",
        "             elif isinstance(first_value, pd.Timestamp):\n",
        "                 inferred_type = pl.Date\n",
        "\n",
        "        # print(f\"Debug: Inferred type for column '{col}': {inferred_type}\")\n",
        "\n",
        "        # Attempt to cast the column to the inferred type\n",
        "        try:\n",
        "            if inferred_type == pl.Date:\n",
        "                # Cast to Utf8 first if not already, then attempt parsing to Datetime\n",
        "                if new_df[col].dtype != pl.Utf8:\n",
        "                     new_df = new_df.with_columns(pl.col(col).cast(pl.Utf8).alias(col))\n",
        "\n",
        "                # Parse as Datetime and then cast to Date\n",
        "                new_df = new_df.with_columns(pl.col(col).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\", strict=False).cast(pl.Date).alias(col))\n",
        "                # print(f\"Debug: Column '{col}' successfully casted to Date.\")\n",
        "            elif inferred_type == pl.Float64:\n",
        "                 # Attempt to cast to Float64, coercing errors to null\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(pl.Float64, strict=False).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to Float64.\")\n",
        "            elif new_df[col].dtype != inferred_type: # Avoid unnecessary recasting\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(inferred_type).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to {inferred_type}.\")\n",
        "            # else:\n",
        "            #     print(f\"Debug: Column '{col}' already has the inferred type {inferred_type}, no casting needed.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"Debug: Could not cast column '{col}' to {inferred_type}: {e}. Keeping original dtype.\")\n",
        "            # If casting fails, keep original dtype or Utf8 if that was the original\n",
        "            pass # Do nothing, keep the current dtype\n",
        "\n",
        "\n",
        "    print(\"✅ Polars DataFrame dtype validation complete.\")\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\n",
        "# Ensure the cleaned_folder exists\n",
        "cleaned_folder = \"cleaned_polars\"\n",
        "if not os.path.exists(cleaned_folder):\n",
        "    os.makedirs(cleaned_folder)\n",
        "\n",
        "# Iterate through files in the current directory\n",
        "start_time = time.time()\n",
        "for filename in os.listdir(\".\"):\n",
        "    if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):\n",
        "        print(f\"\\nProcessing {filename} with Polars...\")\n",
        "        try:\n",
        "            # Read the excel file using Polars with FastExcel warper\n",
        "            df = pl.read_excel(filename)\n",
        "            # Clean the dataframe\n",
        "            cleaned_df_pl = cleaning_df_polars(df)\n",
        "\n",
        "            if cleaned_df_pl is not None:\n",
        "                # Define the output filename\n",
        "                output_filename = os.path.join(cleaned_folder, f\"cleaned_polars_{filename}\")\n",
        "\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(\"✅ Polars DataFrame cleaning complete.\")\n",
        "                print(f\"Time cleaning: {elapsed_time:.4f} seconds\")\n",
        "                start_time = time.time()\n",
        "                #validate dtype of Polars DF because the FastExcel return all column as String\n",
        "                cleaned_df_pl=polars_dataframe_dtype_validate(cleaned_df_pl)\n",
        "                #write to excel\n",
        "                print(\"Saving...\")\n",
        "                cleaned_df_pl.write_excel(output_filename)\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(\"✅ Polars DataFrame cleaning complete.\")\n",
        "                print(f\"Time exporting to excel: {elapsed_time:.4f} seconds\")\n",
        "                print(f\"Saved cleaned Polars data to {output_filename}\")\n",
        "            else:\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(\"✅ Polars DataFrame cleaning complete.\")\n",
        "                print(f\"Time elapsed: {elapsed_time:.4f} seconds\")\n",
        "                print(f\"Skipping saving for {filename} due to cleaning failure.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            print(\"✅ Polars DataFrame cleaning complete.\")\n",
        "            print(f\"Time elapsed: {elapsed_time:.4f} seconds\")\n",
        "            print(f\"Error processing {filename} with Polars: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df_pl.head(20)\n"
      ],
      "metadata": {
        "id": "vtGSXKCW8RtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Excel Data Cleaner (Powered by Polars)\n",
        "\n",
        "# @markdown This notebook helps you clean messy Excel files by automatically detecting headers, cleaning column names,\n",
        "# @markdown dropping empty rows and columns, and inferring data types.\n",
        "# @markdown It processes all `.xlsx` and `.xls` files in the current Google Colab directory.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### How to Use:\n",
        "# @markdown 1. **Upload your Excel files:** Drag and drop your `.xlsx` or `.xls` files directly into the Google Colab file browser (left sidebar -> folder icon).\n",
        "# @markdown 2. **Run the code cell:** Click the \"Play\" button next to this code cell, or press `Ctrl + Enter` (Windows/Linux) / `Cmd + Enter` (Mac).\n",
        "# @markdown 3. **Monitor the output:** The cell will display progress messages and indicate when cleaning is complete.\n",
        "# @markdown 4. **Download cleaned files:** A new folder named `cleaned_polars` will be created in your Colab environment,\n",
        "# @markdown    containing the cleaned versions of your Excel files (e.g., `cleaned_polars_yourfile.xlsx`).\n",
        "# @markdown    You can download them from the file browser.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Configuration:\n",
        "# @markdown Adjust these parameters if the default behavior isn't suitable for your data.\n",
        "\n",
        "min_header_non_nulls=3 # @param {type:\"integer\"}\n",
        "# @markdown Minimum number of non-null values required in a row to be considered a potential header row.\n",
        "# @markdown Increase this if your header rows are sparse, decrease if they are very dense and some data rows might be mistaken for headers.\n",
        "\n",
        "min_row_non_null_percentage=0.25 # @param {type:\"number\"}\n",
        "# @markdown Minimum percentage (0.0 to 1.0) of non-null values a row must have to avoid being dropped.\n",
        "# @markdown Rows with fewer non-nulls than this percentage will be considered \"mostly empty\" and removed.\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Important Considerations:\n",
        "# @markdown * **File Location:** Ensure your Excel files are in the *same directory* as this notebook in Google Colab.\n",
        "# @markdown * **Large Files:** For very large Excel files, the processing time might be significant.\n",
        "# @markdown * **Date Format:** The date inference currently attempts to parse dates in `YYYY-MM-DD HH:MM:SS` format. If your dates are in a different format, they might remain as strings or be incorrectly parsed. Future improvements could include more robust date parsing.\n",
        "# @markdown * **Error Handling:** While some error handling is in place, highly malformed Excel files might still cause issues.\n",
        "# @markdown * **Output Folder:** Cleaned files are saved in a subfolder named `cleaned_polars`. If you run this multiple times, existing files with the same name in this folder will be overwritten.\n",
        "\n",
        "\n",
        "%pip install -q fastexcel xlsxwriter polars\n",
        "import os\n",
        "import polars as pl\n",
        "import time\n",
        "import re\n",
        "import logging\n",
        "logging.getLogger('fastexcel.types.dtype').disabled = True\n",
        "\n",
        "def find_header_row_pl(df_preview: pl.DataFrame, min_non_nulls: int) -> int | None:\n",
        "    \"\"\"\n",
        "    Finds the index of the potential header row in a Polars DataFrame preview.\n",
        "    A row is considered a header row if it has at least `min_non_nulls` non-null values.\n",
        "    \"\"\"\n",
        "    for idx in range(df_preview.height):\n",
        "        row = df_preview.row(idx, named=False)\n",
        "        non_null_count = sum(1 for x in row if x is not None and str(x).strip() != '')\n",
        "        if non_null_count >= min_non_nulls:\n",
        "            return idx\n",
        "    return None\n",
        "\n",
        "def _clean_column_names_pl(columns: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Cleans a list of column names for use in Polars DataFrame.\n",
        "    Replaces non-alphanumeric characters with underscores, removes leading/trailing underscores,\n",
        "    collapses multiple underscores, and handles duplicates by appending a number.\n",
        "    \"\"\"\n",
        "    cleaned = [col.strip().lower() if col is not None else \"\" for col in columns]\n",
        "    cleaned = [re.sub(r\"[^\\w]+\", \"_\", col) for col in cleaned]\n",
        "    cleaned = [col.strip(\"_\") for col in cleaned]\n",
        "    cleaned = [re.sub(r\"_{2,}\", \"_\", col) for col in cleaned]\n",
        "\n",
        "    counts = {}\n",
        "    cleaned_final = []\n",
        "    for col in cleaned:\n",
        "        if col in counts:\n",
        "            counts[col] += 1\n",
        "            cleaned_final.append(f\"{col}_{counts[col]}\")\n",
        "            print(f\"  ↪ Renaming duplicate column '{col}' to '{col}_{counts[col]}'\")\n",
        "        else:\n",
        "            counts[col] = 0\n",
        "            cleaned_final.append(col)\n",
        "    return cleaned_final\n",
        "\n",
        "\n",
        "def cleaning_df_polars(df: pl.DataFrame, min_header_non_nulls: int, min_row_non_null_percentage: float) -> pl.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Cleans a Polars DataFrame that originated from an Excel file.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "        min_header_non_nulls: Minimum non-nulls for header detection.\n",
        "        min_row_non_null_percentage: Minimum percentage of non-nulls for a row to be kept.\n",
        "\n",
        "    Returns:\n",
        "        A cleaned Polars DataFrame or None if cleaning fails.\n",
        "    \"\"\"\n",
        "    print(\"  Starting Polars DataFrame cleaning...\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Find header row\n",
        "        df_preview = df.head(20) # Look at the first 20 rows for header detection\n",
        "        header_row_idx = find_header_row_pl(df_preview, min_header_non_nulls)\n",
        "\n",
        "        if header_row_idx is None:\n",
        "            print(f\"  ⚠️ Could not find a valid header in the provided DataFrame. Skipping cleaning.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"  ✅ Detected header row at index: {header_row_idx}\")\n",
        "\n",
        "        # Step 2: Extract data from the detected header row onwards\n",
        "        headers = df.row(header_row_idx, named=False)\n",
        "        cleaned_columns = _clean_column_names_pl(headers)\n",
        "\n",
        "        df_data = df.slice(offset=header_row_idx + 1, length=df.height - (header_row_idx + 1))\n",
        "\n",
        "        rename_dict = {old_col: new_col for old_col, new_col in zip(df_data.columns, cleaned_columns)}\n",
        "        df_data = df_data.rename(rename_dict)\n",
        "        print(f\"  ✅ Columns renamed and data extracted.\")\n",
        "\n",
        "        # Step 3: Clean data - Drop fully empty columns, drop mostly empty rows, ffill\n",
        "        cols_to_drop = [col for col in df_data.columns if df_data[col].is_null().all() or (df_data[col] == '').all()]\n",
        "        if cols_to_drop:\n",
        "            print(f\"  🗑️ Dropping fully empty columns: {', '.join(cols_to_drop)}\")\n",
        "            df_data = df_data.drop(cols_to_drop)\n",
        "\n",
        "        # Drop mostly empty rows (rows with less than min_row_non_null_percentage non-null values)\n",
        "        min_non_null_per_row = int(df_data.shape[1] * min_row_non_null_percentage)\n",
        "        print(f\"  🗑️ Dropping rows with less than {min_non_null_per_row} non-null values (based on {min_row_non_null_percentage*100}% threshold).\")\n",
        "\n",
        "        non_null_exprs = [\n",
        "            pl.when(pl.col(c).is_not_null() & (pl.col(c).cast(pl.Utf8) != '')).then(1).otherwise(0)\n",
        "            for c in df_data.columns\n",
        "        ]\n",
        "\n",
        "        df_data = df_data.filter(\n",
        "            pl.sum_horizontal(non_null_exprs) >= min_non_null_per_row\n",
        "        )\n",
        "        print(f\"  ✅ Remaining rows after dropping mostly empty: {df_data.shape[0]}\")\n",
        "\n",
        "        # Forward-fill missing values\n",
        "        df_data = df_data.fill_null(strategy=\"forward\")\n",
        "        print(\"  ✅ Forward-filled missing values.\")\n",
        "\n",
        "        return df_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error cleaning Polars DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "def polars_dataframe_dtype_validate(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Infers the data type for each column in a Polars DataFrame based on its content\n",
        "    and casts the column to the inferred type.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A Polars DataFrame with column types validated and casted based on content.\n",
        "    \"\"\"\n",
        "    print(\"Starting Polars DataFrame dtype validation...\")\n",
        "    new_df = df.clone() # Create a copy to avoid modifying the original DataFrame in place\n",
        "\n",
        "    for col in new_df.columns:\n",
        "        # print(f\"Debug: Processing column '{col}'...\")\n",
        "        # Skip if the column is entirely null\n",
        "        if new_df[col].is_null().all():\n",
        "            # print(f\"Debug: Column '{col}' is all null, keeping as Utf8.\")\n",
        "            continue # Keep the default Utf8 type for all-null columns\n",
        "\n",
        "        # Get the first non-null, non-empty string value to infer type\n",
        "        first_value = None\n",
        "        for value in new_df[col].to_list():\n",
        "            if value is not None and value != '' and (isinstance(value, str) and value.strip() != '' or not isinstance(value, str)):\n",
        "                 first_value = value\n",
        "                 break\n",
        "\n",
        "        inferred_type = pl.Utf8 # Default type\n",
        "\n",
        "        if first_value is not None:\n",
        "             # Try to infer a more specific type\n",
        "             if isinstance(first_value, (int, float)):\n",
        "                 inferred_type = pl.Float64\n",
        "             elif isinstance(first_value, str):\n",
        "                 try:\n",
        "                     # Attempt to parse as date\n",
        "                     # Use an explicit format string based on the observed data\n",
        "                     pl.from_epoch(new_df[col].str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\"), time_unit=\"ms\") # Check if can be parsed as Datetime first\n",
        "                     inferred_type = pl.Date # If successful, we can cast to Date\n",
        "                 except Exception: # Catch any exception during parsing\n",
        "                     try:\n",
        "                         # Attempt to parse as a number (float)\n",
        "                         float(first_value)\n",
        "                         inferred_type = pl.Float64\n",
        "                     except (ValueError, TypeError):\n",
        "                          # If neither date nor number, keep as string\n",
        "                         inferred_type = pl.Utf8\n",
        "             elif isinstance(first_value, pd.Timestamp):\n",
        "                 inferred_type = pl.Date\n",
        "\n",
        "        # print(f\"Debug: Inferred type for column '{col}': {inferred_type}\")\n",
        "\n",
        "        # Attempt to cast the column to the inferred type\n",
        "        try:\n",
        "            if inferred_type == pl.Date:\n",
        "                # Cast to Utf8 first if not already, then attempt parsing to Datetime\n",
        "                if new_df[col].dtype != pl.Utf8:\n",
        "                     new_df = new_df.with_columns(pl.col(col).cast(pl.Utf8).alias(col))\n",
        "\n",
        "                # Parse as Datetime and then cast to Date\n",
        "                new_df = new_df.with_columns(pl.col(col).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\", strict=False).cast(pl.Date).alias(col))\n",
        "                # print(f\"Debug: Column '{col}' successfully casted to Date.\")\n",
        "            elif inferred_type == pl.Float64:\n",
        "                 # Attempt to cast to Float64, coercing errors to null\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(pl.Float64, strict=False).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to Float64.\")\n",
        "            elif new_df[col].dtype != inferred_type: # Avoid unnecessary recasting\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(inferred_type).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to {inferred_type}.\")\n",
        "            # else:\n",
        "            #     print(f\"Debug: Column '{col}' already has the inferred type {inferred_type}, no casting needed.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"Debug: Could not cast column '{col}' to {inferred_type}: {e}. Keeping original dtype.\")\n",
        "            # If casting fails, keep original dtype or Utf8 if that was the original\n",
        "            pass # Do nothing, keep the current dtype\n",
        "\n",
        "\n",
        "    print(\"✅ Polars DataFrame dtype validation complete.\")\n",
        "    return new_df\n",
        "\n",
        "\n",
        "# Ensure the cleaned_folder exists\n",
        "cleaned_folder = \"cleaned_polars\"\n",
        "if not os.path.exists(cleaned_folder):\n",
        "    os.makedirs(cleaned_folder)\n",
        "    print(f\"Created output folder: {cleaned_folder}\")\n",
        "\n",
        "print(\"\\n--- Starting Excel Cleaning Process ---\")\n",
        "total_start_time = time.time()\n",
        "processed_files_count = 0\n",
        "\n",
        "# Iterate through files in the current directory\n",
        "for filename in os.listdir(\".\"):\n",
        "    if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):\n",
        "        processed_files_count += 1\n",
        "        print(f\"\\n✨ Processing file: {filename}...\")\n",
        "        file_start_time = time.time()\n",
        "        try:\n",
        "            # Read the excel file using Polars with FastExcel wrapper\n",
        "            # Use `read_excel_fastexcel` to ensure the wrapper is used for performance\n",
        "            df = pl.read_excel(filename) # Specify engine for clarity\n",
        "\n",
        "            # Clean the dataframe\n",
        "            cleaned_df_pl = cleaning_df_polars(df, min_header_non_nulls, min_row_non_null_percentage)\n",
        "\n",
        "            if cleaned_df_pl is not None:\n",
        "                # Define the output filename\n",
        "                output_filename = os.path.join(cleaned_folder, f\"cleaned_polars_{filename}\")\n",
        "\n",
        "                # Validate and cast dtypes\n",
        "                cleaned_df_pl = polars_dataframe_dtype_validate(cleaned_df_pl)\n",
        "\n",
        "                # Write to excel\n",
        "                print(\"  💾 Saving cleaned DataFrame...\")\n",
        "                cleaned_df_pl.write_excel(output_filename) # xlsxwriter is needed for writing\n",
        "\n",
        "                file_end_time = time.time()\n",
        "                elapsed_file_time = file_end_time - file_start_time\n",
        "                print(f\"  ✅ Successfully processed and saved to {output_filename} in {elapsed_file_time:.2f} seconds.\")\n",
        "            else:\n",
        "                file_end_time = time.time()\n",
        "                elapsed_file_time = file_end_time - file_start_time\n",
        "                print(f\"  ❌ Skipping saving for {filename} due to cleaning failure (took {elapsed_file_time:.2f} seconds).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            file_end_time = time.time()\n",
        "            elapsed_file_time = file_end_time - file_start_time\n",
        "            print(f\"  ❌ Error processing {filename}: {e} (took {elapsed_file_time:.2f} seconds).\")\n",
        "\n",
        "if processed_files_count == 0:\n",
        "    print(\"\\nNo Excel files (.xlsx or .xls) found in the current directory to process.\")\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_elapsed_time = total_end_time - total_start_time\n",
        "print(f\"\\n--- Excel Cleaning Process Complete! Total time: {total_elapsed_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wYDOcNYY-_ix",
        "outputId": "6a7ac634-03b6-43f0-bd95-73e64551c744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Excel Cleaning Process ---\n",
            "\n",
            "✨ Processing file: rincian_faktur_penjualan_ptgoeboekinfota_250428102742.xlsx...\n",
            "  Starting Polars DataFrame cleaning...\n",
            "  ✅ Detected header row at index: 2\n",
            "  ↪ Renaming duplicate column 'diskon' to 'diskon_1'\n",
            "  ↪ Renaming duplicate column 'diskon' to 'diskon_2'\n",
            "  ↪ Renaming duplicate column 'diskon' to 'diskon_3'\n",
            "  ↪ Renaming duplicate column '' to '_1'\n",
            "  ✅ Columns renamed and data extracted.\n",
            "  🗑️ Dropping rows with less than 6 non-null values (based on 25.0% threshold).\n",
            "  ✅ Remaining rows after dropping mostly empty: 130278\n",
            "  ✅ Forward-filled missing values.\n",
            "Starting Polars DataFrame dtype validation...\n",
            "✅ Polars DataFrame dtype validation complete.\n",
            "  💾 Saving cleaned DataFrame...\n",
            "  ✅ Successfully processed and saved to cleaned_polars/cleaned_polars_rincian_faktur_penjualan_ptgoeboekinfota_250428102742.xlsx in 56.95 seconds.\n",
            "\n",
            "--- Excel Cleaning Process Complete! Total time: 56.95 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cleaned excel preview (first 10 rows)\n",
        "cleaned_df_pl.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "cellView": "form",
        "id": "bEbhs3F3BSvo",
        "outputId": "62771beb-cca5-45c2-f452-6ce5680fc5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10, 27)\n",
              "┌──────┬────────────┬─────────────┬─────────────┬───┬─────────────┬────────────┬────────────┬──────┐\n",
              "│      ┆ tanggal    ┆ pelanggan   ┆ nomor       ┆ … ┆ total_terma ┆ kena_pajak ┆ tipe_pajak ┆ _1   │\n",
              "│ ---  ┆ ---        ┆ ---         ┆ ---         ┆   ┆ suk_pajak_f ┆ _faktur_pe ┆ _ppn_baran ┆ ---  │\n",
              "│ str  ┆ date       ┆ str         ┆ str         ┆   ┆ aktur_pe…   ┆ njualan    ┆ g_jasa     ┆ str  │\n",
              "│      ┆            ┆             ┆             ┆   ┆ ---         ┆ ---        ┆ ---        ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆ str         ┆ str        ┆ str        ┆      │\n",
              "╞══════╪════════════╪═════════════╪═════════════╪═══╪═════════════╪════════════╪════════════╪══════╡\n",
              "│ null ┆ 2024-02-02 ┆ YOGYA       ┆ GIT.2024.02 ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆ KEPATIHAN   ┆ .00014      ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "│ null ┆ 2024-02-03 ┆ TIKTOK      ┆ OL.2024.02. ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆             ┆ 00192       ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "│ null ┆ 2024-02-02 ┆ YOGYA       ┆ GIT.2024.02 ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆ SUBANG      ┆ .00008      ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "│ null ┆ 2024-02-02 ┆ YOGYA       ┆ GIT.2024.02 ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆ SUBANG      ┆ .00008      ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "│ null ┆ 2024-02-03 ┆ TIKTOK      ┆ OL.2024.02. ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆             ┆ 00235       ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "│ null ┆ 2024-02-02 ┆ SHOPEE OL   ┆ OL.2024.02. ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆             ┆ 00066       ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "│ null ┆ 2024-02-02 ┆ SHOPEE OL   ┆ OL.2024.02. ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆             ┆ 00066       ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "│ null ┆ 2024-02-05 ┆ TIKTOK      ┆ OL.2024.02. ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆             ┆ 00304       ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "│ null ┆ 2024-02-03 ┆ YOGYA       ┆ GIT.2024.02 ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆ SUBANG      ┆ .00025      ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "│ null ┆ 2024-02-03 ┆ SHOPEE OL   ┆ OL.2024.02. ┆ … ┆ Ya          ┆ Ya         ┆ Pajak Pert ┆ null │\n",
              "│      ┆            ┆             ┆ 00147       ┆   ┆             ┆            ┆ ambahan    ┆      │\n",
              "│      ┆            ┆             ┆             ┆   ┆             ┆            ┆ Nilai      ┆      │\n",
              "└──────┴────────────┴─────────────┴─────────────┴───┴─────────────┴────────────┴────────────┴──────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 27)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>tanggal</th><th>pelanggan</th><th>nomor</th><th>kategori</th><th>kode</th><th>nama_barang</th><th>gudang</th><th>kuantitas</th><th>harga</th><th>diskon</th><th>diskon_1</th><th>diskon_2</th><th>diskon_3</th><th>total_penjualan_setelah_discount_inc_ppn</th><th>total_sebelum_diskon_per_faktur</th><th>bpp_inc_ppn</th><th>bpp_qty_inc_ppn</th><th>laba_satuan_inc_ppn</th><th>laba_kotor_total</th><th>no_order</th><th>sales</th><th>keterangan</th><th>total_termasuk_pajak_faktur_penjualan</th><th>kena_pajak_faktur_penjualan</th><th>tipe_pajak_ppn_barang_jasa</th><th>_1</th></tr><tr><td>str</td><td>date</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>null</td><td>2024-02-02</td><td>&quot;YOGYA KEPATIHAN&quot;</td><td>&quot;GIT.2024.02.00014&quot;</td><td>&quot;OLIKE ACC&quot;</td><td>&quot;LDU SF4 BLUE&quot;</td><td>&quot;LDU OLIKE BEATZ WIRELESS PORTA…</td><td>&quot;YOGYA KEPATIHAN&quot;</td><td>1.0</td><td>143200.0</td><td>17900.0</td><td>12.5</td><td>0.0</td><td>null</td><td>125300.13</td><td>125300.0</td><td>83399.9943</td><td>83399.9943</td><td>41900.1357</td><td>41900.1357</td><td>null</td><td>&quot;CC-CHRISTIAN CHANDRA&quot;</td><td>null</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-03</td><td>&quot;TIKTOK&quot;</td><td>&quot;OL.2024.02.00192&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU ADDCPM30&quot;</td><td>&quot;MAKUKU DRY CARE PANTS M30&quot;</td><td>&quot;ONLINE&quot;</td><td>1.0</td><td>44299.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>44300.1</td><td>44299.0</td><td>43919.925</td><td>43919.925</td><td>380.175</td><td>380.175</td><td>5.7847e17</td><td>&quot;TK2FN-FEIZAL&quot;</td><td>null</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-02</td><td>&quot;YOGYA SUBANG&quot;</td><td>&quot;GIT.2024.02.00008&quot;</td><td>&quot;OLIKE ACC&quot;</td><td>&quot;T112 WHITE&quot;</td><td>&quot;OLIKE TRUE WIRELESS EARPHONES …</td><td>&quot;YOGYA SUBANG&quot;</td><td>1.0</td><td>144560.0</td><td>38970.0</td><td>12.5</td><td>0.0</td><td>null</td><td>126490.27</td><td>126490.0007</td><td>104999.99934</td><td>104999.99934</td><td>21490.270659</td><td>21490.27066</td><td>5.7847e17</td><td>&quot;CC-CHRISTIAN CHANDRA&quot;</td><td>&quot;01 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-02</td><td>&quot;YOGYA SUBANG&quot;</td><td>&quot;GIT.2024.02.00008&quot;</td><td>&quot;OLIKE ACC&quot;</td><td>&quot;P101S DARK BLUE&quot;</td><td>&quot;OLIKE DIGITAL DISPL POWERBANK1…</td><td>&quot;YOGYA SUBANG&quot;</td><td>1.0</td><td>167200.0</td><td>38970.0</td><td>12.5</td><td>0.0</td><td>null</td><td>146300.0</td><td>146300.03827</td><td>136999.99953</td><td>136999.99953</td><td>9300.00047</td><td>9300.00047</td><td>5.7847e17</td><td>&quot;CC-CHRISTIAN CHANDRA&quot;</td><td>&quot;01 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-03</td><td>&quot;TIKTOK&quot;</td><td>&quot;OL.2024.02.00235&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU ADDCP L28&quot;</td><td>&quot;MAKUKU DRY CARE PANTS L28&quot;</td><td>&quot;ONLINE&quot;</td><td>1.0</td><td>44299.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>44300.1</td><td>44299.0</td><td>43919.925</td><td>43919.925</td><td>380.175</td><td>380.175</td><td>5.7848e17</td><td>&quot;TK2FN-FEIZAL&quot;</td><td>&quot;01 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-02</td><td>&quot;SHOPEE OL&quot;</td><td>&quot;OL.2024.02.00066&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU TSM ADSPXL32N&quot;</td><td>&quot;MAKUKU TSM SLIM PANTS XL32 NEW&quot;</td><td>&quot;ONLINE&quot;</td><td>5.0</td><td>102600.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>513000.329999</td><td>513000.0</td><td>94856.16</td><td>474280.8</td><td>7743.905999</td><td>38719.529999</td><td>null</td><td>&quot;SP1FN-FEIZAL&quot;</td><td>&quot;INSTAN&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-02</td><td>&quot;SHOPEE OL&quot;</td><td>&quot;OL.2024.02.00066&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU AD SCP XL46&quot;</td><td>&quot;MAKUKU SLIM CARE PANTS XL46&quot;</td><td>&quot;ONLINE&quot;</td><td>5.0</td><td>120999.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>604995.000001</td><td>604995.0</td><td>108083.808</td><td>540419.04</td><td>12915.192</td><td>64575.960001</td><td>null</td><td>&quot;SP1FN-FEIZAL&quot;</td><td>&quot;INSTAN&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-05</td><td>&quot;TIKTOK&quot;</td><td>&quot;OL.2024.02.00304&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU ADDCPM30&quot;</td><td>&quot;MAKUKU DRY CARE PANTS M30&quot;</td><td>&quot;ONLINE&quot;</td><td>1.0</td><td>44299.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>44300.1</td><td>44299.0</td><td>43919.925</td><td>43919.925</td><td>380.175</td><td>380.175</td><td>5.7848e17</td><td>&quot;TK2FN-FEIZAL&quot;</td><td>&quot;INSTAN&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-03</td><td>&quot;YOGYA SUBANG&quot;</td><td>&quot;GIT.2024.02.00025&quot;</td><td>&quot;OLIKE ACC&quot;</td><td>&quot;OLIKE CPH HC1 BLACK&quot;</td><td>&quot;OLIKE CAR PHONE HOLDER HC1 BLA…</td><td>&quot;YOGYA SUBANG&quot;</td><td>1.0</td><td>69520.0</td><td>8690.0</td><td>12.5</td><td>0.0</td><td>null</td><td>60830.22</td><td>60830.0</td><td>45999.998658</td><td>45999.998659</td><td>14830.221341</td><td>14830.221341</td><td>5.7848e17</td><td>&quot;CC-CHRISTIAN CHANDRA&quot;</td><td>&quot;03 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-03</td><td>&quot;SHOPEE OL&quot;</td><td>&quot;OL.2024.02.00147&quot;</td><td>&quot;KLAR ACC&quot;</td><td>&quot;KL-DBG002&quot;</td><td>&quot;KLAR LAUNDRY DETERGENT BALLS B…</td><td>&quot;ONLINE&quot;</td><td>1.0</td><td>11384.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>11384.32</td><td>11384.0</td><td>10319.448</td><td>10319.448</td><td>1064.871999</td><td>1064.872</td><td>null</td><td>&quot;SP1FN-FEIZAL&quot;</td><td>&quot;03 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}