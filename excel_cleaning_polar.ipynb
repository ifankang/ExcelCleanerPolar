{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORMTmAq3BY2zkAdRizujxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifankang/ExcelCleanerPolar/blob/main/excel_cleaning_polar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Excel Data Cleaner (Powered by Polars)\n",
        "\n",
        "# @markdown This notebook helps you clean messy Excel files by automatically detecting headers, cleaning column names,\n",
        "# @markdown dropping empty rows and columns, and inferring data types.\n",
        "# @markdown It processes all `.xlsx` and `.xls` files in the current Google Colab directory.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### How to Use:\n",
        "# @markdown 1. **Upload your Excel files:** Drag and drop your `.xlsx` or `.xls` files directly into the Google Colab file browser (left sidebar -> folder icon).\n",
        "# @markdown 2. **Run the code cell:** Click the \"Play\" button next to this code cell, or press `Ctrl + Enter` (Windows/Linux) / `Cmd + Enter` (Mac).\n",
        "# @markdown 3. **Monitor the output:** The cell will display progress messages and indicate when cleaning is complete.\n",
        "# @markdown 4. **Download cleaned files:** A new folder named `cleaned_polars` will be created in your Colab environment,\n",
        "# @markdown    containing the cleaned versions of your Excel files (e.g., `cleaned_polars_yourfile.xlsx`).\n",
        "# @markdown    You can download them from the file browser.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Configuration:\n",
        "# @markdown Adjust these parameters if the default behavior isn't suitable for your data.\n",
        "\n",
        "min_header_non_nulls=3 # @param {type:\"integer\"}\n",
        "# @markdown Minimum number of non-null values required in a row to be considered a potential header row.\n",
        "# @markdown Increase this if your header rows are sparse, decrease if they are very dense and some data rows might be mistaken for headers.\n",
        "\n",
        "min_row_non_null_percentage=0.25 # @param {type:\"number\"}\n",
        "# @markdown Minimum percentage (0.0 to 1.0) of non-null values a row must have to avoid being dropped.\n",
        "# @markdown Rows with fewer non-nulls than this percentage will be considered \"mostly empty\" and removed.\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Important Considerations:\n",
        "# @markdown * **File Location:** Ensure your Excel files are in the *same directory* as this notebook in Google Colab.\n",
        "# @markdown * **Large Files:** For very large Excel files, the processing time might be significant.\n",
        "# @markdown * **Date Format:** The date inference currently attempts to parse dates in `YYYY-MM-DD HH:MM:SS` format. If your dates are in a different format, they might remain as strings or be incorrectly parsed. Future improvements could include more robust date parsing.\n",
        "# @markdown * **Error Handling:** While some error handling is in place, highly malformed Excel files might still cause issues.\n",
        "# @markdown * **Output Folder:** Cleaned files are saved in a subfolder named `cleaned_polars`. If you run this multiple times, existing files with the same name in this folder will be overwritten.\n",
        "\n",
        "\n",
        "%pip install -q fastexcel xlsxwriter polars\n",
        "import os\n",
        "import polars as pl\n",
        "import time\n",
        "import re\n",
        "import logging\n",
        "logging.getLogger('fastexcel.types.dtype').disabled = True\n",
        "\n",
        "def find_header_row_pl(df_preview: pl.DataFrame, min_non_nulls: int) -> int | None:\n",
        "    \"\"\"\n",
        "    Finds the index of the potential header row in a Polars DataFrame preview.\n",
        "    A row is considered a header row if it has at least `min_non_nulls` non-null values.\n",
        "    \"\"\"\n",
        "    for idx in range(df_preview.height):\n",
        "        row = df_preview.row(idx, named=False)\n",
        "        non_null_count = sum(1 for x in row if x is not None and str(x).strip() != '')\n",
        "        if non_null_count >= min_non_nulls:\n",
        "            return idx\n",
        "    return None\n",
        "\n",
        "def _clean_column_names_pl(columns: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Cleans a list of column names for use in Polars DataFrame.\n",
        "    Replaces non-alphanumeric characters with underscores, removes leading/trailing underscores,\n",
        "    collapses multiple underscores, and handles duplicates by appending a number.\n",
        "    \"\"\"\n",
        "    cleaned = [col.strip().lower() if col is not None else \"\" for col in columns]\n",
        "    cleaned = [re.sub(r\"[^\\w]+\", \"_\", col) for col in cleaned]\n",
        "    cleaned = [col.strip(\"_\") for col in cleaned]\n",
        "    cleaned = [re.sub(r\"_{2,}\", \"_\", col) for col in cleaned]\n",
        "\n",
        "    counts = {}\n",
        "    cleaned_final = []\n",
        "    for col in cleaned:\n",
        "        if col in counts:\n",
        "            counts[col] += 1\n",
        "            cleaned_final.append(f\"{col}_{counts[col]}\")\n",
        "            print(f\"  ‚Ü™ Renaming duplicate column '{col}' to '{col}_{counts[col]}'\")\n",
        "        else:\n",
        "            counts[col] = 0\n",
        "            cleaned_final.append(col)\n",
        "    return cleaned_final\n",
        "\n",
        "\n",
        "def cleaning_df_polars(df: pl.DataFrame, min_header_non_nulls: int, min_row_non_null_percentage: float) -> pl.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Cleans a Polars DataFrame that originated from an Excel file.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "        min_header_non_nulls: Minimum non-nulls for header detection.\n",
        "        min_row_non_null_percentage: Minimum percentage of non-nulls for a row to be kept.\n",
        "\n",
        "    Returns:\n",
        "        A cleaned Polars DataFrame or None if cleaning fails.\n",
        "    \"\"\"\n",
        "    print(\"  Starting Polars DataFrame cleaning...\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Find header row\n",
        "        df_preview = df.head(20) # Look at the first 20 rows for header detection\n",
        "        header_row_idx = find_header_row_pl(df_preview, min_header_non_nulls)\n",
        "\n",
        "        if header_row_idx is None:\n",
        "            print(f\"  ‚ö†Ô∏è Could not find a valid header in the provided DataFrame. Skipping cleaning.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"  ‚úÖ Detected header row at index: {header_row_idx}\")\n",
        "\n",
        "        # Step 2: Extract data from the detected header row onwards\n",
        "        headers = df.row(header_row_idx, named=False)\n",
        "        cleaned_columns = _clean_column_names_pl(headers)\n",
        "\n",
        "        df_data = df.slice(offset=header_row_idx + 1, length=df.height - (header_row_idx + 1))\n",
        "\n",
        "        rename_dict = {old_col: new_col for old_col, new_col in zip(df_data.columns, cleaned_columns)}\n",
        "        df_data = df_data.rename(rename_dict)\n",
        "        print(f\"  ‚úÖ Columns renamed and data extracted.\")\n",
        "\n",
        "        # Step 3: Clean data - Drop fully empty columns, drop mostly empty rows, ffill\n",
        "        cols_to_drop = [col for col in df_data.columns if df_data[col].is_null().all() or (df_data[col] == '').all()]\n",
        "        if cols_to_drop:\n",
        "            print(f\"  üóëÔ∏è Dropping fully empty columns: {', '.join(cols_to_drop)}\")\n",
        "            df_data = df_data.drop(cols_to_drop)\n",
        "\n",
        "        # Drop mostly empty rows (rows with less than min_row_non_null_percentage non-null values)\n",
        "        min_non_null_per_row = int(df_data.shape[1] * min_row_non_null_percentage)\n",
        "        print(f\"  üóëÔ∏è Dropping rows with less than {min_non_null_per_row} non-null values (based on {min_row_non_null_percentage*100}% threshold).\")\n",
        "\n",
        "        non_null_exprs = [\n",
        "            pl.when(pl.col(c).is_not_null() & (pl.col(c).cast(pl.Utf8) != '')).then(1).otherwise(0)\n",
        "            for c in df_data.columns\n",
        "        ]\n",
        "\n",
        "        df_data = df_data.filter(\n",
        "            pl.sum_horizontal(non_null_exprs) >= min_non_null_per_row\n",
        "        )\n",
        "        print(f\"  ‚úÖ Remaining rows after dropping mostly empty: {df_data.shape[0]}\")\n",
        "\n",
        "        # Forward-fill missing values\n",
        "        df_data = df_data.fill_null(strategy=\"forward\")\n",
        "        print(\"  ‚úÖ Forward-filled missing values.\")\n",
        "\n",
        "        return df_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error cleaning Polars DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "def polars_dataframe_dtype_validate(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Infers the data type for each column in a Polars DataFrame based on its content\n",
        "    and casts the column to the inferred type.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A Polars DataFrame with column types validated and casted based on content.\n",
        "    \"\"\"\n",
        "    print(\"Starting Polars DataFrame dtype validation...\")\n",
        "    new_df = df.clone() # Create a copy to avoid modifying the original DataFrame in place\n",
        "\n",
        "    for col in new_df.columns:\n",
        "        # print(f\"Debug: Processing column '{col}'...\")\n",
        "        # Skip if the column is entirely null\n",
        "        if new_df[col].is_null().all():\n",
        "            # print(f\"Debug: Column '{col}' is all null, keeping as Utf8.\")\n",
        "            continue # Keep the default Utf8 type for all-null columns\n",
        "\n",
        "        # Get the first non-null, non-empty string value to infer type\n",
        "        first_value = None\n",
        "        for value in new_df[col].to_list():\n",
        "            if value is not None and value != '' and (isinstance(value, str) and value.strip() != '' or not isinstance(value, str)):\n",
        "                 first_value = value\n",
        "                 break\n",
        "\n",
        "        inferred_type = pl.Utf8 # Default type\n",
        "\n",
        "        if first_value is not None:\n",
        "             # Try to infer a more specific type\n",
        "             if isinstance(first_value, (int, float)):\n",
        "                 inferred_type = pl.Float64\n",
        "             elif isinstance(first_value, str):\n",
        "                 try:\n",
        "                     # Attempt to parse as date\n",
        "                     # Use an explicit format string based on the observed data\n",
        "                     pl.from_epoch(new_df[col].str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\"), time_unit=\"ms\") # Check if can be parsed as Datetime first\n",
        "                     inferred_type = pl.Date # If successful, we can cast to Date\n",
        "                 except Exception: # Catch any exception during parsing\n",
        "                     try:\n",
        "                         # Attempt to parse as a number (float)\n",
        "                         float(first_value)\n",
        "                         inferred_type = pl.Float64\n",
        "                     except (ValueError, TypeError):\n",
        "                          # If neither date nor number, keep as string\n",
        "                         inferred_type = pl.Utf8\n",
        "             elif isinstance(first_value, pd.Timestamp):\n",
        "                 inferred_type = pl.Date\n",
        "\n",
        "        # print(f\"Debug: Inferred type for column '{col}': {inferred_type}\")\n",
        "\n",
        "        # Attempt to cast the column to the inferred type\n",
        "        try:\n",
        "            if inferred_type == pl.Date:\n",
        "                # Cast to Utf8 first if not already, then attempt parsing to Datetime\n",
        "                if new_df[col].dtype != pl.Utf8:\n",
        "                     new_df = new_df.with_columns(pl.col(col).cast(pl.Utf8).alias(col))\n",
        "\n",
        "                # Parse as Datetime and then cast to Date\n",
        "                new_df = new_df.with_columns(pl.col(col).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\", strict=False).cast(pl.Date).alias(col))\n",
        "                # print(f\"Debug: Column '{col}' successfully casted to Date.\")\n",
        "            elif inferred_type == pl.Float64:\n",
        "                 # Attempt to cast to Float64, coercing errors to null\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(pl.Float64, strict=False).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to Float64.\")\n",
        "            elif new_df[col].dtype != inferred_type: # Avoid unnecessary recasting\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(inferred_type).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to {inferred_type}.\")\n",
        "            # else:\n",
        "            #     print(f\"Debug: Column '{col}' already has the inferred type {inferred_type}, no casting needed.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"Debug: Could not cast column '{col}' to {inferred_type}: {e}. Keeping original dtype.\")\n",
        "            # If casting fails, keep original dtype or Utf8 if that was the original\n",
        "            pass # Do nothing, keep the current dtype\n",
        "\n",
        "\n",
        "    print(\"‚úÖ Polars DataFrame dtype validation complete.\")\n",
        "    return new_df\n",
        "\n",
        "\n",
        "# Ensure the cleaned_folder exists\n",
        "cleaned_folder = \"cleaned_polars\"\n",
        "if not os.path.exists(cleaned_folder):\n",
        "    os.makedirs(cleaned_folder)\n",
        "    print(f\"Created output folder: {cleaned_folder}\")\n",
        "\n",
        "print(\"\\n--- Starting Excel Cleaning Process ---\")\n",
        "total_start_time = time.time()\n",
        "processed_files_count = 0\n",
        "\n",
        "# Iterate through files in the current directory\n",
        "for filename in os.listdir(\".\"):\n",
        "    if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):\n",
        "        processed_files_count += 1\n",
        "        print(f\"\\n‚ú® Processing file: {filename}...\")\n",
        "        file_start_time = time.time()\n",
        "        try:\n",
        "            # Read the excel file using Polars with FastExcel wrapper\n",
        "            # Use `read_excel_fastexcel` to ensure the wrapper is used for performance\n",
        "            df = pl.read_excel(filename) # Specify engine for clarity\n",
        "\n",
        "            # Clean the dataframe\n",
        "            cleaned_df_pl = cleaning_df_polars(df, min_header_non_nulls, min_row_non_null_percentage)\n",
        "\n",
        "            if cleaned_df_pl is not None:\n",
        "                # Define the output filename\n",
        "                output_filename = os.path.join(cleaned_folder, f\"cleaned_polars_{filename}\")\n",
        "\n",
        "                # Validate and cast dtypes\n",
        "                cleaned_df_pl = polars_dataframe_dtype_validate(cleaned_df_pl)\n",
        "\n",
        "                # Write to excel\n",
        "                print(\"  üíæ Saving cleaned DataFrame...\")\n",
        "                cleaned_df_pl.write_excel(output_filename) # xlsxwriter is needed for writing\n",
        "\n",
        "                file_end_time = time.time()\n",
        "                elapsed_file_time = file_end_time - file_start_time\n",
        "                print(f\"  ‚úÖ Successfully processed and saved to {output_filename} in {elapsed_file_time:.2f} seconds.\")\n",
        "            else:\n",
        "                file_end_time = time.time()\n",
        "                elapsed_file_time = file_end_time - file_start_time\n",
        "                print(f\"  ‚ùå Skipping saving for {filename} due to cleaning failure (took {elapsed_file_time:.2f} seconds).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            file_end_time = time.time()\n",
        "            elapsed_file_time = file_end_time - file_start_time\n",
        "            print(f\"  ‚ùå Error processing {filename}: {e} (took {elapsed_file_time:.2f} seconds).\")\n",
        "\n",
        "if processed_files_count == 0:\n",
        "    print(\"\\nNo Excel files (.xlsx or .xls) found in the current directory to process.\")\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_elapsed_time = total_end_time - total_start_time\n",
        "print(f\"\\n--- Excel Cleaning Process Complete! Total time: {total_elapsed_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wYDOcNYY-_ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cleaned excel preview (first 10 rows)\n",
        "cleaned_df_pl.head(10)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bEbhs3F3BSvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}