{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTuV7736F7EG2oa90PblDF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifankang/ExcelCleanerPolar/blob/main/excel_cleaning_polar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Clean Excel Files\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def find_header_row(df_preview, min_non_nulls=3):\n",
        "    for idx, row in df_preview.iterrows():\n",
        "        if row.notna().sum() >= min_non_nulls:\n",
        "            return idx\n",
        "    return None\n",
        "\n",
        "def _clean_column_names(columns):\n",
        "    cleaned = pd.Series(columns).astype(str).str.strip().str.lower()\n",
        "    cleaned = cleaned.str.replace(r\"[^\\w]+\", \"_\", regex=True).str.strip(\"_\")\n",
        "    cleaned = cleaned.str.replace(r\"_{2,}\", \"_\", regex=True)\n",
        "\n",
        "    for dup in cleaned[cleaned.duplicated()].unique():\n",
        "        idxs = cleaned[cleaned == dup].index\n",
        "        cleaned.loc[idxs] = [f\"{dup}_{i}\" if i else dup for i in range(len(idxs))]\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "def cleaning_df(df: pd.DataFrame) -> pd.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Cleans a pandas DataFrame that originated from an Excel file.\n",
        "\n",
        "    Args:\n",
        "        df: The input pandas DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A cleaned pandas DataFrame or None if cleaning fails.\n",
        "    \"\"\"\n",
        "    print(\"Starting DataFrame cleaning...\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Step 1: Find header row (using the find_header_row logic adapted for DataFrame)\n",
        "        # We can't use a file path anymore, so we need a way to detect the header row\n",
        "        # within the DataFrame itself. We'll look at the first few rows.\n",
        "        df_preview = df.head(20).copy() # Look at the first 20 rows for header detection\n",
        "\n",
        "        header_row_idx = None\n",
        "        # Assuming the header is a row where a reasonable number of columns are not null\n",
        "        # This is a heuristic and might need adjustment based on the specific data structure\n",
        "        for idx in range(len(df_preview)):\n",
        "             # Check if at least 3 non-null values exist in this row\n",
        "            if df_preview.iloc[idx].notna().sum() >= 3:\n",
        "                header_row_idx = idx\n",
        "                break\n",
        "\n",
        "\n",
        "        if header_row_idx is None:\n",
        "            print(f\"âš ï¸ Could not find valid header in the provided DataFrame.\")\n",
        "            return None\n",
        "\n",
        "        # # Step 2: Extract data from the detected header row onwards\n",
        "        # # The actual headers are in the detected header row (header_row_idx)\n",
        "        headers = df.iloc[header_row_idx].tolist()\n",
        "        # # The data starts from the row *after* the header row\n",
        "        df_data = df.iloc[header_row_idx+1:].copy()\n",
        "\n",
        "        df_data.columns = headers\n",
        "        df_data = df_data.dropna(how=\"all\", axis=1)     # Drop fully empty columns\n",
        "        # Drop mostly empty rows (rows with less than 50% non-null values)\n",
        "        df_data = df_data.dropna(thresh=int(df_data.shape[1] * 0.5))\n",
        "        df_data = df_data.ffill()                       # Forward-fill missing values\n",
        "\n",
        "\n",
        "        # Step 3: Clean column names to remove duplicates and unsafe characters\n",
        "        df_data.columns = _clean_column_names(df_data.columns)\n",
        "\n",
        "\n",
        "        print(\"âœ… DataFrame cleaning complete.\")\n",
        "        return df_data.reset_index(drop=True)\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print(f\"âŒ Error cleaning DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "# Ensure the cleaned_folder exists\n",
        "start_time = time.time()\n",
        "cleaned_folder = \"cleaned\"\n",
        "if not os.path.exists(cleaned_folder):\n",
        "    os.makedirs(cleaned_folder)\n",
        "\n",
        "# Iterate through files in the current directory\n",
        "for filename in os.listdir(\".\"):\n",
        "    if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):\n",
        "        print(f\"Processing {filename}...\")\n",
        "        try:\n",
        "            # Read the excel file\n",
        "            df = pd.read_excel(filename, engine='calamine')\n",
        "\n",
        "            # Clean the dataframe\n",
        "            cleaned_df = cleaning_df(df.copy())\n",
        "\n",
        "            if cleaned_df is not None:\n",
        "                # Define the output filename\n",
        "                output_filename = os.path.join(cleaned_folder, f\"cleaned_{filename}\")\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(f\"Time elapsed: {elapsed_time:.4f} seconds\")\n",
        "                start_time = time.time()\n",
        "                # Save the cleaned dataframe to a new excel file\n",
        "                cleaned_df.to_excel(output_filename, index=False, engine='xlsxwriter')\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(f\"Time exporting to excel: {elapsed_time:.4f} seconds\")\n",
        "                print(f\"Saved cleaned data to {output_filename}\")\n",
        "            else:\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(f\"Time elapsed: {elapsed_time:.4f} seconds\")\n",
        "                print(f\"Skipping saving for {filename} due to cleaning failure.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            print(f\"Time elapsed before error: {elapsed_time:.4f} seconds\")\n",
        "            print(f\"Error processing {filename}: {e}\")"
      ],
      "metadata": {
        "id": "yGdVUrg7fuQI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df.head(20)"
      ],
      "metadata": {
        "id": "ik8Kq9eb0mYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkWgclMKaW0X"
      },
      "outputs": [],
      "source": [
        "# @title Clean Excel (Faster)\n",
        "%pip install -q fastexcel xlsxwriter polars\n",
        "import os\n",
        "import polars as pl\n",
        "import time\n",
        "import re\n",
        "import logging\n",
        "logging.getLogger('fastexcel.types.dtype').disabled = True\n",
        "\n",
        "def find_header_row_pl(df_preview: pl.DataFrame, min_non_nulls=3) -> int | None:\n",
        "    \"\"\"\n",
        "    Finds the index of the potential header row in a Polars DataFrame preview.\n",
        "    A row is considered a header row if it has at least `min_non_nulls` non-null values.\n",
        "    \"\"\"\n",
        "    # print(\"Debug: Finding header row...\")\n",
        "    for idx in range(df_preview.height):\n",
        "        row = df_preview.row(idx, named=False) # Get row as a tuple\n",
        "        # Count non-nulls in the tuple\n",
        "        non_null_count = sum(1 for x in row if x is not None and x != '') # Add check for empty strings\n",
        "        # print(f\"Debug: Row {idx}: Non-null count = {non_null_count}\")\n",
        "        if non_null_count >= min_non_nulls:\n",
        "            # print(f\"Debug: Found potential header row at index {idx}\")\n",
        "            return idx\n",
        "    # print(\"Debug: No valid header row found.\")\n",
        "    return None\n",
        "\n",
        "def _clean_column_names_pl(columns: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Cleans a list of column names for use in Polars DataFrame.\n",
        "    Replaces non-alphanumeric characters with underscores, removes leading/trailing underscores,\n",
        "    collapses multiple underscores, and handles duplicates by appending a number.\n",
        "    \"\"\"\n",
        "    # print(\"Debug: Cleaning column names...\")\n",
        "    cleaned = [col.strip().lower() if col is not None else \"\" for col in columns] # Handle None values\n",
        "    cleaned = [re.sub(r\"[^\\w]+\", \"_\", col) for col in cleaned]\n",
        "    cleaned = [col.strip(\"_\") for col in cleaned]\n",
        "    cleaned = [re.sub(r\"_{2,}\", \"_\", col) for col in cleaned]\n",
        "\n",
        "    counts = {}\n",
        "    cleaned_final = []\n",
        "    for col in cleaned:\n",
        "        if col in counts:\n",
        "            counts[col] += 1\n",
        "            cleaned_final.append(f\"{col}_{counts[col]}\")\n",
        "            print(f\"Debug: Renaming duplicate column '{col}' to '{col}_{counts[col]}'\")\n",
        "        else:\n",
        "            counts[col] = 0\n",
        "            cleaned_final.append(col)\n",
        "            # print(f\"Debug: Keeping column name '{col}'\")\n",
        "\n",
        "    # print(\"Debug: Column names cleaned.\")\n",
        "    return cleaned_final\n",
        "\n",
        "\n",
        "def cleaning_df_polars(df: pl.DataFrame) -> pl.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Cleans a Polars DataFrame that originated from an Excel file.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A cleaned Polars DataFrame or None if cleaning fails.\n",
        "    \"\"\"\n",
        "    print(\"Starting Polars DataFrame cleaning...\")\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Step 1: Find header row\n",
        "        df_preview = df.head(20) # Look at the first 20 rows for header detection\n",
        "        header_row_idx = find_header_row_pl(df_preview)\n",
        "\n",
        "        if header_row_idx is None:\n",
        "            print(f\"âš ï¸ Could not find valid header in the provided DataFrame.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "        # Step 2: Extract data from the detected header row onwards\n",
        "        # The actual headers are in the detected header row (header_row_idx)\n",
        "        headers = df.row(header_row_idx, named=False)\n",
        "        cleaned_columns = _clean_column_names_pl(headers)\n",
        "        # print(f\"Debug: Cleaned Headers: {cleaned_columns}\")\n",
        "\n",
        "\n",
        "        # The data starts from the row *after* the header row\n",
        "        # Slice the DataFrame first\n",
        "        df_data = df.slice(offset=header_row_idx + 1, length=df.height - (header_row_idx + 1))\n",
        "\n",
        "\n",
        "\n",
        "        # Now rename the sliced DataFrame using the cleaned headers\n",
        "        # Create a dictionary mapping the original column names of df_data to the cleaned column names\n",
        "        # The original column names are still the __UNNAMED ones from pl.read_excel\n",
        "        rename_dict = {old_col: new_col for old_col, new_col in zip(df_data.columns, cleaned_columns)}\n",
        "        df_data = df_data.rename(rename_dict)\n",
        "        # print(f\"Debug: Rename dictionary: {rename_dict}\")\n",
        "        # print(f\"Debug: DataFrame after renaming columns and slicing data. Shape: {df_data.shape}\")\n",
        "        # print(f\"Debug: Columns after renaming: {df_data.columns}\")\n",
        "\n",
        "        # Step 4: Clean data - Drop fully empty columns, drop mostly empty rows, ffill\n",
        "        # Drop fully empty columns (check if all values are null or empty string)\n",
        "        cols_to_drop = [col for col in df_data.columns if df_data[col].is_null().all() or (df_data[col] == '').all()]\n",
        "        if cols_to_drop:\n",
        "            # print(f\"Debug: Dropping fully empty columns: {cols_to_drop}\")\n",
        "            df_data = df_data.drop(cols_to_drop)\n",
        "            # print(f\"Debug: DataFrame shape after dropping empty columns: {df_data.shape}\")\n",
        "\n",
        "        # Drop mostly empty rows (rows with less than 25% non-null values)\n",
        "        # Calculate minimum non-null values required per row\n",
        "        min_non_null_per_row = int(df_data.shape[1] * 0.25)\n",
        "        print(f\"Debug: Dropping rows with less than {min_non_null_per_row} non-null values.\")\n",
        "\n",
        "        # Calculate row-wise non-null counts, excluding empty strings\n",
        "        # Use a list comprehension to create expressions for each column\n",
        "        non_null_exprs = [\n",
        "            pl.when(pl.col(c).is_not_null() & (pl.col(c).cast(pl.Utf8) != '')).then(1).otherwise(0)\n",
        "            for c in df_data.columns\n",
        "        ]\n",
        "\n",
        "        # Sum the non-null counts across columns for each row and filter\n",
        "        df_data = df_data.filter(\n",
        "            pl.sum_horizontal(non_null_exprs) >= min_non_null_per_row\n",
        "        )\n",
        "\n",
        "\n",
        "        # Remove completely empty rows\n",
        "        # df_data = df_data.filter(\n",
        "        #   pl.concat_list([pl.col(col).is_not_null() & (pl.col(col) != \"\") for col in df.columns])\n",
        "        # ).any(axis=1)\n",
        "\n",
        "        # Drop mostly empty rows (rows with less than 50% non-null values\n",
        "        # # Calculate minimum non-null values required per row\n",
        "        # min_non_null_per_row = int(df_data.shape[1] * 0.5)\n",
        "        # print(f\"Debug: Dropping rows with less than {min_non_null_per_row} non-null values.\")\n",
        "\n",
        "        # # Calculate row-wise non-null counts, excluding empty strings\n",
        "        # row_non_null_counts = df_data.select([\n",
        "        #     (pl.when(pl.col(c).is_not_null() & (pl.col(c).cast(pl.Utf8) != '')).then(1).otherwise(0)).sum().alias(c) for c in df_data.columns\n",
        "        # ]).sum().to_series()  # Convert to Series\n",
        "\n",
        "        # # Print non-null counts for debugging\n",
        "        # print(\"Debug: Non-null counts per row:\")\n",
        "        # print(row_non_null_counts)\n",
        "\n",
        "        # # Filter rows based on the calculated non-null counts\n",
        "        # df_data = df_data.filter(row_non_null_counts >= min_non_null_per_row)\n",
        "        # print(f\"Debug: DataFrame shape after dropping mostly empty rows: {df_data.shape}\")\n",
        "\n",
        "\n",
        "        # Forward-fill missing values\n",
        "        # Polars ffill works column-wise\n",
        "        df_data = df_data.fill_null(strategy=\"forward\")\n",
        "        # Also consider filling empty strings with the previous non-empty value if needed\n",
        "        # This is more complex and depends on the data type and desired behavior.\n",
        "        # For simplicity, fill nulls only.\n",
        "        return df_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error cleaning Polars DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "def polars_dataframe_dtype_validate(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Infers the data type for each column in a Polars DataFrame based on its content\n",
        "    and casts the column to the inferred type.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A Polars DataFrame with column types validated and casted based on content.\n",
        "    \"\"\"\n",
        "    print(\"Starting Polars DataFrame dtype validation...\")\n",
        "    new_df = df.clone() # Create a copy to avoid modifying the original DataFrame in place\n",
        "\n",
        "    for col in new_df.columns:\n",
        "        # print(f\"Debug: Processing column '{col}'...\")\n",
        "        # Skip if the column is entirely null\n",
        "        if new_df[col].is_null().all():\n",
        "            # print(f\"Debug: Column '{col}' is all null, keeping as Utf8.\")\n",
        "            continue # Keep the default Utf8 type for all-null columns\n",
        "\n",
        "        # Get the first non-null, non-empty string value to infer type\n",
        "        first_value = None\n",
        "        for value in new_df[col].to_list():\n",
        "            if value is not None and value != '' and (isinstance(value, str) and value.strip() != '' or not isinstance(value, str)):\n",
        "                 first_value = value\n",
        "                 break\n",
        "\n",
        "        inferred_type = pl.Utf8 # Default type\n",
        "\n",
        "        if first_value is not None:\n",
        "             # Try to infer a more specific type\n",
        "             if isinstance(first_value, (int, float)):\n",
        "                 inferred_type = pl.Float64\n",
        "             elif isinstance(first_value, str):\n",
        "                 try:\n",
        "                     # Attempt to parse as date\n",
        "                     # Use an explicit format string based on the observed data\n",
        "                     pl.from_epoch(new_df[col].str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\"), time_unit=\"ms\") # Check if can be parsed as Datetime first\n",
        "                     inferred_type = pl.Date # If successful, we can cast to Date\n",
        "                 except Exception: # Catch any exception during parsing\n",
        "                     try:\n",
        "                         # Attempt to parse as a number (float)\n",
        "                         float(first_value)\n",
        "                         inferred_type = pl.Float64\n",
        "                     except (ValueError, TypeError):\n",
        "                          # If neither date nor number, keep as string\n",
        "                         inferred_type = pl.Utf8\n",
        "             elif isinstance(first_value, pd.Timestamp):\n",
        "                 inferred_type = pl.Date\n",
        "\n",
        "        # print(f\"Debug: Inferred type for column '{col}': {inferred_type}\")\n",
        "\n",
        "        # Attempt to cast the column to the inferred type\n",
        "        try:\n",
        "            if inferred_type == pl.Date:\n",
        "                # Cast to Utf8 first if not already, then attempt parsing to Datetime\n",
        "                if new_df[col].dtype != pl.Utf8:\n",
        "                     new_df = new_df.with_columns(pl.col(col).cast(pl.Utf8).alias(col))\n",
        "\n",
        "                # Parse as Datetime and then cast to Date\n",
        "                new_df = new_df.with_columns(pl.col(col).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\", strict=False).cast(pl.Date).alias(col))\n",
        "                # print(f\"Debug: Column '{col}' successfully casted to Date.\")\n",
        "            elif inferred_type == pl.Float64:\n",
        "                 # Attempt to cast to Float64, coercing errors to null\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(pl.Float64, strict=False).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to Float64.\")\n",
        "            elif new_df[col].dtype != inferred_type: # Avoid unnecessary recasting\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(inferred_type).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to {inferred_type}.\")\n",
        "            # else:\n",
        "            #     print(f\"Debug: Column '{col}' already has the inferred type {inferred_type}, no casting needed.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"Debug: Could not cast column '{col}' to {inferred_type}: {e}. Keeping original dtype.\")\n",
        "            # If casting fails, keep original dtype or Utf8 if that was the original\n",
        "            pass # Do nothing, keep the current dtype\n",
        "\n",
        "\n",
        "    print(\"âœ… Polars DataFrame dtype validation complete.\")\n",
        "    return new_df\n",
        "\n",
        "\n",
        "\n",
        "# Ensure the cleaned_folder exists\n",
        "cleaned_folder = \"cleaned_polars\"\n",
        "if not os.path.exists(cleaned_folder):\n",
        "    os.makedirs(cleaned_folder)\n",
        "\n",
        "# Iterate through files in the current directory\n",
        "start_time = time.time()\n",
        "for filename in os.listdir(\".\"):\n",
        "    if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):\n",
        "        print(f\"\\nProcessing {filename} with Polars...\")\n",
        "        try:\n",
        "            # Read the excel file using Polars with FastExcel warper\n",
        "            df = pl.read_excel(filename)\n",
        "            # Clean the dataframe\n",
        "            cleaned_df_pl = cleaning_df_polars(df)\n",
        "\n",
        "            if cleaned_df_pl is not None:\n",
        "                # Define the output filename\n",
        "                output_filename = os.path.join(cleaned_folder, f\"cleaned_polars_{filename}\")\n",
        "\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(\"âœ… Polars DataFrame cleaning complete.\")\n",
        "                print(f\"Time cleaning: {elapsed_time:.4f} seconds\")\n",
        "                start_time = time.time()\n",
        "                #validate dtype of Polars DF because the FastExcel return all column as String\n",
        "                cleaned_df_pl=polars_dataframe_dtype_validate(cleaned_df_pl)\n",
        "                #write to excel\n",
        "                print(\"Saving...\")\n",
        "                cleaned_df_pl.write_excel(output_filename)\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(\"âœ… Polars DataFrame cleaning complete.\")\n",
        "                print(f\"Time exporting to excel: {elapsed_time:.4f} seconds\")\n",
        "                print(f\"Saved cleaned Polars data to {output_filename}\")\n",
        "            else:\n",
        "                end_time = time.time()\n",
        "                elapsed_time = end_time - start_time\n",
        "                print(\"âœ… Polars DataFrame cleaning complete.\")\n",
        "                print(f\"Time elapsed: {elapsed_time:.4f} seconds\")\n",
        "                print(f\"Skipping saving for {filename} due to cleaning failure.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            end_time = time.time()\n",
        "            elapsed_time = end_time - start_time\n",
        "            print(\"âœ… Polars DataFrame cleaning complete.\")\n",
        "            print(f\"Time elapsed: {elapsed_time:.4f} seconds\")\n",
        "            print(f\"Error processing {filename} with Polars: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df_pl.head(20)\n"
      ],
      "metadata": {
        "id": "vtGSXKCW8RtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Excel Data Cleaner (Powered by Polars)\n",
        "\n",
        "# @markdown This notebook helps you clean messy Excel files by automatically detecting headers, cleaning column names,\n",
        "# @markdown dropping empty rows and columns, and inferring data types.\n",
        "# @markdown It processes all `.xlsx` and `.xls` files in the current Google Colab directory.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### How to Use:\n",
        "# @markdown 1. **Upload your Excel files:** Drag and drop your `.xlsx` or `.xls` files directly into the Google Colab file browser (left sidebar -> folder icon).\n",
        "# @markdown 2. **Run the code cell:** Click the \"Play\" button next to this code cell, or press `Ctrl + Enter` (Windows/Linux) / `Cmd + Enter` (Mac).\n",
        "# @markdown 3. **Monitor the output:** The cell will display progress messages and indicate when cleaning is complete.\n",
        "# @markdown 4. **Download cleaned files:** A new folder named `cleaned_polars` will be created in your Colab environment,\n",
        "# @markdown    containing the cleaned versions of your Excel files (e.g., `cleaned_polars_yourfile.xlsx`).\n",
        "# @markdown    You can download them from the file browser.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Configuration:\n",
        "# @markdown Adjust these parameters if the default behavior isn't suitable for your data.\n",
        "\n",
        "min_header_non_nulls=3 # @param {type:\"integer\"}\n",
        "# @markdown Minimum number of non-null values required in a row to be considered a potential header row.\n",
        "# @markdown Increase this if your header rows are sparse, decrease if they are very dense and some data rows might be mistaken for headers.\n",
        "\n",
        "min_row_non_null_percentage=0.25 # @param {type:\"number\"}\n",
        "# @markdown Minimum percentage (0.0 to 1.0) of non-null values a row must have to avoid being dropped.\n",
        "# @markdown Rows with fewer non-nulls than this percentage will be considered \"mostly empty\" and removed.\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### Important Considerations:\n",
        "# @markdown * **File Location:** Ensure your Excel files are in the *same directory* as this notebook in Google Colab.\n",
        "# @markdown * **Large Files:** For very large Excel files, the processing time might be significant.\n",
        "# @markdown * **Date Format:** The date inference currently attempts to parse dates in `YYYY-MM-DD HH:MM:SS` format. If your dates are in a different format, they might remain as strings or be incorrectly parsed. Future improvements could include more robust date parsing.\n",
        "# @markdown * **Error Handling:** While some error handling is in place, highly malformed Excel files might still cause issues.\n",
        "# @markdown * **Output Folder:** Cleaned files are saved in a subfolder named `cleaned_polars`. If you run this multiple times, existing files with the same name in this folder will be overwritten.\n",
        "\n",
        "\n",
        "%pip install -q fastexcel xlsxwriter polars\n",
        "import os\n",
        "import polars as pl\n",
        "import time\n",
        "import re\n",
        "import logging\n",
        "logging.getLogger('fastexcel.types.dtype').disabled = True\n",
        "\n",
        "def find_header_row_pl(df_preview: pl.DataFrame, min_non_nulls: int) -> int | None:\n",
        "    \"\"\"\n",
        "    Finds the index of the potential header row in a Polars DataFrame preview.\n",
        "    A row is considered a header row if it has at least `min_non_nulls` non-null values.\n",
        "    \"\"\"\n",
        "    for idx in range(df_preview.height):\n",
        "        row = df_preview.row(idx, named=False)\n",
        "        non_null_count = sum(1 for x in row if x is not None and str(x).strip() != '')\n",
        "        if non_null_count >= min_non_nulls:\n",
        "            return idx\n",
        "    return None\n",
        "\n",
        "def _clean_column_names_pl(columns: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Cleans a list of column names for use in Polars DataFrame.\n",
        "    Replaces non-alphanumeric characters with underscores, removes leading/trailing underscores,\n",
        "    collapses multiple underscores, and handles duplicates by appending a number.\n",
        "    \"\"\"\n",
        "    cleaned = [col.strip().lower() if col is not None else \"\" for col in columns]\n",
        "    cleaned = [re.sub(r\"[^\\w]+\", \"_\", col) for col in cleaned]\n",
        "    cleaned = [col.strip(\"_\") for col in cleaned]\n",
        "    cleaned = [re.sub(r\"_{2,}\", \"_\", col) for col in cleaned]\n",
        "\n",
        "    counts = {}\n",
        "    cleaned_final = []\n",
        "    for col in cleaned:\n",
        "        if col in counts:\n",
        "            counts[col] += 1\n",
        "            cleaned_final.append(f\"{col}_{counts[col]}\")\n",
        "            print(f\"  â†ª Renaming duplicate column '{col}' to '{col}_{counts[col]}'\")\n",
        "        else:\n",
        "            counts[col] = 0\n",
        "            cleaned_final.append(col)\n",
        "    return cleaned_final\n",
        "\n",
        "\n",
        "def cleaning_df_polars(df: pl.DataFrame, min_header_non_nulls: int, min_row_non_null_percentage: float) -> pl.DataFrame | None:\n",
        "    \"\"\"\n",
        "    Cleans a Polars DataFrame that originated from an Excel file.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "        min_header_non_nulls: Minimum non-nulls for header detection.\n",
        "        min_row_non_null_percentage: Minimum percentage of non-nulls for a row to be kept.\n",
        "\n",
        "    Returns:\n",
        "        A cleaned Polars DataFrame or None if cleaning fails.\n",
        "    \"\"\"\n",
        "    print(\"  Starting Polars DataFrame cleaning...\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Find header row\n",
        "        df_preview = df.head(20) # Look at the first 20 rows for header detection\n",
        "        header_row_idx = find_header_row_pl(df_preview, min_header_non_nulls)\n",
        "\n",
        "        if header_row_idx is None:\n",
        "            print(f\"  âš ï¸ Could not find a valid header in the provided DataFrame. Skipping cleaning.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"  âœ… Detected header row at index: {header_row_idx}\")\n",
        "\n",
        "        # Step 2: Extract data from the detected header row onwards\n",
        "        headers = df.row(header_row_idx, named=False)\n",
        "        cleaned_columns = _clean_column_names_pl(headers)\n",
        "\n",
        "        df_data = df.slice(offset=header_row_idx + 1, length=df.height - (header_row_idx + 1))\n",
        "\n",
        "        rename_dict = {old_col: new_col for old_col, new_col in zip(df_data.columns, cleaned_columns)}\n",
        "        df_data = df_data.rename(rename_dict)\n",
        "        print(f\"  âœ… Columns renamed and data extracted.\")\n",
        "\n",
        "        # Step 3: Clean data - Drop fully empty columns, drop mostly empty rows, ffill\n",
        "        cols_to_drop = [col for col in df_data.columns if df_data[col].is_null().all() or (df_data[col] == '').all()]\n",
        "        if cols_to_drop:\n",
        "            print(f\"  ğŸ—‘ï¸ Dropping fully empty columns: {', '.join(cols_to_drop)}\")\n",
        "            df_data = df_data.drop(cols_to_drop)\n",
        "\n",
        "        # Drop mostly empty rows (rows with less than min_row_non_null_percentage non-null values)\n",
        "        min_non_null_per_row = int(df_data.shape[1] * min_row_non_null_percentage)\n",
        "        print(f\"  ğŸ—‘ï¸ Dropping rows with less than {min_non_null_per_row} non-null values (based on {min_row_non_null_percentage*100}% threshold).\")\n",
        "\n",
        "        non_null_exprs = [\n",
        "            pl.when(pl.col(c).is_not_null() & (pl.col(c).cast(pl.Utf8) != '')).then(1).otherwise(0)\n",
        "            for c in df_data.columns\n",
        "        ]\n",
        "\n",
        "        df_data = df_data.filter(\n",
        "            pl.sum_horizontal(non_null_exprs) >= min_non_null_per_row\n",
        "        )\n",
        "        print(f\"  âœ… Remaining rows after dropping mostly empty: {df_data.shape[0]}\")\n",
        "\n",
        "        # Forward-fill missing values\n",
        "        df_data = df_data.fill_null(strategy=\"forward\")\n",
        "        print(\"  âœ… Forward-filled missing values.\")\n",
        "\n",
        "        return df_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ Error cleaning Polars DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "def polars_dataframe_dtype_validate(df: pl.DataFrame) -> pl.DataFrame:\n",
        "    \"\"\"\n",
        "    Infers the data type for each column in a Polars DataFrame based on its content\n",
        "    and casts the column to the inferred type.\n",
        "\n",
        "    Args:\n",
        "        df: The input Polars DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        A Polars DataFrame with column types validated and casted based on content.\n",
        "    \"\"\"\n",
        "    print(\"Starting Polars DataFrame dtype validation...\")\n",
        "    new_df = df.clone() # Create a copy to avoid modifying the original DataFrame in place\n",
        "\n",
        "    for col in new_df.columns:\n",
        "        # print(f\"Debug: Processing column '{col}'...\")\n",
        "        # Skip if the column is entirely null\n",
        "        if new_df[col].is_null().all():\n",
        "            # print(f\"Debug: Column '{col}' is all null, keeping as Utf8.\")\n",
        "            continue # Keep the default Utf8 type for all-null columns\n",
        "\n",
        "        # Get the first non-null, non-empty string value to infer type\n",
        "        first_value = None\n",
        "        for value in new_df[col].to_list():\n",
        "            if value is not None and value != '' and (isinstance(value, str) and value.strip() != '' or not isinstance(value, str)):\n",
        "                 first_value = value\n",
        "                 break\n",
        "\n",
        "        inferred_type = pl.Utf8 # Default type\n",
        "\n",
        "        if first_value is not None:\n",
        "             # Try to infer a more specific type\n",
        "             if isinstance(first_value, (int, float)):\n",
        "                 inferred_type = pl.Float64\n",
        "             elif isinstance(first_value, str):\n",
        "                 try:\n",
        "                     # Attempt to parse as date\n",
        "                     # Use an explicit format string based on the observed data\n",
        "                     pl.from_epoch(new_df[col].str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\"), time_unit=\"ms\") # Check if can be parsed as Datetime first\n",
        "                     inferred_type = pl.Date # If successful, we can cast to Date\n",
        "                 except Exception: # Catch any exception during parsing\n",
        "                     try:\n",
        "                         # Attempt to parse as a number (float)\n",
        "                         float(first_value)\n",
        "                         inferred_type = pl.Float64\n",
        "                     except (ValueError, TypeError):\n",
        "                          # If neither date nor number, keep as string\n",
        "                         inferred_type = pl.Utf8\n",
        "             elif isinstance(first_value, pd.Timestamp):\n",
        "                 inferred_type = pl.Date\n",
        "\n",
        "        # print(f\"Debug: Inferred type for column '{col}': {inferred_type}\")\n",
        "\n",
        "        # Attempt to cast the column to the inferred type\n",
        "        try:\n",
        "            if inferred_type == pl.Date:\n",
        "                # Cast to Utf8 first if not already, then attempt parsing to Datetime\n",
        "                if new_df[col].dtype != pl.Utf8:\n",
        "                     new_df = new_df.with_columns(pl.col(col).cast(pl.Utf8).alias(col))\n",
        "\n",
        "                # Parse as Datetime and then cast to Date\n",
        "                new_df = new_df.with_columns(pl.col(col).str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S\", strict=False).cast(pl.Date).alias(col))\n",
        "                # print(f\"Debug: Column '{col}' successfully casted to Date.\")\n",
        "            elif inferred_type == pl.Float64:\n",
        "                 # Attempt to cast to Float64, coercing errors to null\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(pl.Float64, strict=False).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to Float64.\")\n",
        "            elif new_df[col].dtype != inferred_type: # Avoid unnecessary recasting\n",
        "                 new_df = new_df.with_columns(pl.col(col).cast(inferred_type).alias(col))\n",
        "                #  print(f\"Debug: Column '{col}' successfully casted to {inferred_type}.\")\n",
        "            # else:\n",
        "            #     print(f\"Debug: Column '{col}' already has the inferred type {inferred_type}, no casting needed.\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            # print(f\"Debug: Could not cast column '{col}' to {inferred_type}: {e}. Keeping original dtype.\")\n",
        "            # If casting fails, keep original dtype or Utf8 if that was the original\n",
        "            pass # Do nothing, keep the current dtype\n",
        "\n",
        "\n",
        "    print(\"âœ… Polars DataFrame dtype validation complete.\")\n",
        "    return new_df\n",
        "\n",
        "\n",
        "# Ensure the cleaned_folder exists\n",
        "cleaned_folder = \"cleaned_polars\"\n",
        "if not os.path.exists(cleaned_folder):\n",
        "    os.makedirs(cleaned_folder)\n",
        "    print(f\"Created output folder: {cleaned_folder}\")\n",
        "\n",
        "print(\"\\n--- Starting Excel Cleaning Process ---\")\n",
        "total_start_time = time.time()\n",
        "processed_files_count = 0\n",
        "\n",
        "# Iterate through files in the current directory\n",
        "for filename in os.listdir(\".\"):\n",
        "    if filename.endswith(\".xlsx\") or filename.endswith(\".xls\"):\n",
        "        processed_files_count += 1\n",
        "        print(f\"\\nâœ¨ Processing file: {filename}...\")\n",
        "        file_start_time = time.time()\n",
        "        try:\n",
        "            # Read the excel file using Polars with FastExcel wrapper\n",
        "            # Use `read_excel_fastexcel` to ensure the wrapper is used for performance\n",
        "            df = pl.read_excel(filename) # Specify engine for clarity\n",
        "\n",
        "            # Clean the dataframe\n",
        "            cleaned_df_pl = cleaning_df_polars(df, min_header_non_nulls, min_row_non_null_percentage)\n",
        "\n",
        "            if cleaned_df_pl is not None:\n",
        "                # Define the output filename\n",
        "                output_filename = os.path.join(cleaned_folder, f\"cleaned_polars_{filename}\")\n",
        "\n",
        "                # Validate and cast dtypes\n",
        "                cleaned_df_pl = polars_dataframe_dtype_validate(cleaned_df_pl)\n",
        "\n",
        "                # Write to excel\n",
        "                print(\"  ğŸ’¾ Saving cleaned DataFrame...\")\n",
        "                cleaned_df_pl.write_excel(output_filename) # xlsxwriter is needed for writing\n",
        "\n",
        "                file_end_time = time.time()\n",
        "                elapsed_file_time = file_end_time - file_start_time\n",
        "                print(f\"  âœ… Successfully processed and saved to {output_filename} in {elapsed_file_time:.2f} seconds.\")\n",
        "            else:\n",
        "                file_end_time = time.time()\n",
        "                elapsed_file_time = file_end_time - file_start_time\n",
        "                print(f\"  âŒ Skipping saving for {filename} due to cleaning failure (took {elapsed_file_time:.2f} seconds).\")\n",
        "\n",
        "        except Exception as e:\n",
        "            file_end_time = time.time()\n",
        "            elapsed_file_time = file_end_time - file_start_time\n",
        "            print(f\"  âŒ Error processing {filename}: {e} (took {elapsed_file_time:.2f} seconds).\")\n",
        "\n",
        "if processed_files_count == 0:\n",
        "    print(\"\\nNo Excel files (.xlsx or .xls) found in the current directory to process.\")\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_elapsed_time = total_end_time - total_start_time\n",
        "print(f\"\\n--- Excel Cleaning Process Complete! Total time: {total_elapsed_time:.2f} seconds ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wYDOcNYY-_ix",
        "outputId": "6a7ac634-03b6-43f0-bd95-73e64551c744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Excel Cleaning Process ---\n",
            "\n",
            "âœ¨ Processing file: rincian_faktur_penjualan_ptgoeboekinfota_250428102742.xlsx...\n",
            "  Starting Polars DataFrame cleaning...\n",
            "  âœ… Detected header row at index: 2\n",
            "  â†ª Renaming duplicate column 'diskon' to 'diskon_1'\n",
            "  â†ª Renaming duplicate column 'diskon' to 'diskon_2'\n",
            "  â†ª Renaming duplicate column 'diskon' to 'diskon_3'\n",
            "  â†ª Renaming duplicate column '' to '_1'\n",
            "  âœ… Columns renamed and data extracted.\n",
            "  ğŸ—‘ï¸ Dropping rows with less than 6 non-null values (based on 25.0% threshold).\n",
            "  âœ… Remaining rows after dropping mostly empty: 130278\n",
            "  âœ… Forward-filled missing values.\n",
            "Starting Polars DataFrame dtype validation...\n",
            "âœ… Polars DataFrame dtype validation complete.\n",
            "  ğŸ’¾ Saving cleaned DataFrame...\n",
            "  âœ… Successfully processed and saved to cleaned_polars/cleaned_polars_rincian_faktur_penjualan_ptgoeboekinfota_250428102742.xlsx in 56.95 seconds.\n",
            "\n",
            "--- Excel Cleaning Process Complete! Total time: 56.95 seconds ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cleaned excel preview (first 10 rows)\n",
        "cleaned_df_pl.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "cellView": "form",
        "id": "bEbhs3F3BSvo",
        "outputId": "62771beb-cca5-45c2-f452-6ce5680fc5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "shape: (10, 27)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚      â”† tanggal    â”† pelanggan   â”† nomor       â”† â€¦ â”† total_terma â”† kena_pajak â”† tipe_pajak â”† _1   â”‚\n",
              "â”‚ ---  â”† ---        â”† ---         â”† ---         â”†   â”† suk_pajak_f â”† _faktur_pe â”† _ppn_baran â”† ---  â”‚\n",
              "â”‚ str  â”† date       â”† str         â”† str         â”†   â”† aktur_peâ€¦   â”† njualan    â”† g_jasa     â”† str  â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”† ---         â”† ---        â”† ---        â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”† str         â”† str        â”† str        â”†      â”‚\n",
              "â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•¡\n",
              "â”‚ null â”† 2024-02-02 â”† YOGYA       â”† GIT.2024.02 â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”† KEPATIHAN   â”† .00014      â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â”‚ null â”† 2024-02-03 â”† TIKTOK      â”† OL.2024.02. â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”†             â”† 00192       â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â”‚ null â”† 2024-02-02 â”† YOGYA       â”† GIT.2024.02 â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”† SUBANG      â”† .00008      â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â”‚ null â”† 2024-02-02 â”† YOGYA       â”† GIT.2024.02 â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”† SUBANG      â”† .00008      â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â”‚ null â”† 2024-02-03 â”† TIKTOK      â”† OL.2024.02. â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”†             â”† 00235       â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â”‚ null â”† 2024-02-02 â”† SHOPEE OL   â”† OL.2024.02. â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”†             â”† 00066       â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â”‚ null â”† 2024-02-02 â”† SHOPEE OL   â”† OL.2024.02. â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”†             â”† 00066       â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â”‚ null â”† 2024-02-05 â”† TIKTOK      â”† OL.2024.02. â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”†             â”† 00304       â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â”‚ null â”† 2024-02-03 â”† YOGYA       â”† GIT.2024.02 â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”† SUBANG      â”† .00025      â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â”‚ null â”† 2024-02-03 â”† SHOPEE OL   â”† OL.2024.02. â”† â€¦ â”† Ya          â”† Ya         â”† Pajak Pert â”† null â”‚\n",
              "â”‚      â”†            â”†             â”† 00147       â”†   â”†             â”†            â”† ambahan    â”†      â”‚\n",
              "â”‚      â”†            â”†             â”†             â”†   â”†             â”†            â”† Nilai      â”†      â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (10, 27)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th></th><th>tanggal</th><th>pelanggan</th><th>nomor</th><th>kategori</th><th>kode</th><th>nama_barang</th><th>gudang</th><th>kuantitas</th><th>harga</th><th>diskon</th><th>diskon_1</th><th>diskon_2</th><th>diskon_3</th><th>total_penjualan_setelah_discount_inc_ppn</th><th>total_sebelum_diskon_per_faktur</th><th>bpp_inc_ppn</th><th>bpp_qty_inc_ppn</th><th>laba_satuan_inc_ppn</th><th>laba_kotor_total</th><th>no_order</th><th>sales</th><th>keterangan</th><th>total_termasuk_pajak_faktur_penjualan</th><th>kena_pajak_faktur_penjualan</th><th>tipe_pajak_ppn_barang_jasa</th><th>_1</th></tr><tr><td>str</td><td>date</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>null</td><td>2024-02-02</td><td>&quot;YOGYA KEPATIHAN&quot;</td><td>&quot;GIT.2024.02.00014&quot;</td><td>&quot;OLIKE ACC&quot;</td><td>&quot;LDU SF4 BLUE&quot;</td><td>&quot;LDU OLIKE BEATZ WIRELESS PORTAâ€¦</td><td>&quot;YOGYA KEPATIHAN&quot;</td><td>1.0</td><td>143200.0</td><td>17900.0</td><td>12.5</td><td>0.0</td><td>null</td><td>125300.13</td><td>125300.0</td><td>83399.9943</td><td>83399.9943</td><td>41900.1357</td><td>41900.1357</td><td>null</td><td>&quot;CC-CHRISTIAN CHANDRA&quot;</td><td>null</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-03</td><td>&quot;TIKTOK&quot;</td><td>&quot;OL.2024.02.00192&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU ADDCPM30&quot;</td><td>&quot;MAKUKU DRY CARE PANTS M30&quot;</td><td>&quot;ONLINE&quot;</td><td>1.0</td><td>44299.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>44300.1</td><td>44299.0</td><td>43919.925</td><td>43919.925</td><td>380.175</td><td>380.175</td><td>5.7847e17</td><td>&quot;TK2FN-FEIZAL&quot;</td><td>null</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-02</td><td>&quot;YOGYA SUBANG&quot;</td><td>&quot;GIT.2024.02.00008&quot;</td><td>&quot;OLIKE ACC&quot;</td><td>&quot;T112 WHITE&quot;</td><td>&quot;OLIKE TRUE WIRELESS EARPHONES â€¦</td><td>&quot;YOGYA SUBANG&quot;</td><td>1.0</td><td>144560.0</td><td>38970.0</td><td>12.5</td><td>0.0</td><td>null</td><td>126490.27</td><td>126490.0007</td><td>104999.99934</td><td>104999.99934</td><td>21490.270659</td><td>21490.27066</td><td>5.7847e17</td><td>&quot;CC-CHRISTIAN CHANDRA&quot;</td><td>&quot;01 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-02</td><td>&quot;YOGYA SUBANG&quot;</td><td>&quot;GIT.2024.02.00008&quot;</td><td>&quot;OLIKE ACC&quot;</td><td>&quot;P101S DARK BLUE&quot;</td><td>&quot;OLIKE DIGITAL DISPL POWERBANK1â€¦</td><td>&quot;YOGYA SUBANG&quot;</td><td>1.0</td><td>167200.0</td><td>38970.0</td><td>12.5</td><td>0.0</td><td>null</td><td>146300.0</td><td>146300.03827</td><td>136999.99953</td><td>136999.99953</td><td>9300.00047</td><td>9300.00047</td><td>5.7847e17</td><td>&quot;CC-CHRISTIAN CHANDRA&quot;</td><td>&quot;01 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-03</td><td>&quot;TIKTOK&quot;</td><td>&quot;OL.2024.02.00235&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU ADDCP L28&quot;</td><td>&quot;MAKUKU DRY CARE PANTS L28&quot;</td><td>&quot;ONLINE&quot;</td><td>1.0</td><td>44299.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>44300.1</td><td>44299.0</td><td>43919.925</td><td>43919.925</td><td>380.175</td><td>380.175</td><td>5.7848e17</td><td>&quot;TK2FN-FEIZAL&quot;</td><td>&quot;01 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-02</td><td>&quot;SHOPEE OL&quot;</td><td>&quot;OL.2024.02.00066&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU TSM ADSPXL32N&quot;</td><td>&quot;MAKUKU TSM SLIM PANTS XL32 NEW&quot;</td><td>&quot;ONLINE&quot;</td><td>5.0</td><td>102600.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>513000.329999</td><td>513000.0</td><td>94856.16</td><td>474280.8</td><td>7743.905999</td><td>38719.529999</td><td>null</td><td>&quot;SP1FN-FEIZAL&quot;</td><td>&quot;INSTAN&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-02</td><td>&quot;SHOPEE OL&quot;</td><td>&quot;OL.2024.02.00066&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU AD SCP XL46&quot;</td><td>&quot;MAKUKU SLIM CARE PANTS XL46&quot;</td><td>&quot;ONLINE&quot;</td><td>5.0</td><td>120999.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>604995.000001</td><td>604995.0</td><td>108083.808</td><td>540419.04</td><td>12915.192</td><td>64575.960001</td><td>null</td><td>&quot;SP1FN-FEIZAL&quot;</td><td>&quot;INSTAN&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-05</td><td>&quot;TIKTOK&quot;</td><td>&quot;OL.2024.02.00304&quot;</td><td>&quot;MAKUKU&quot;</td><td>&quot;MAKUKU ADDCPM30&quot;</td><td>&quot;MAKUKU DRY CARE PANTS M30&quot;</td><td>&quot;ONLINE&quot;</td><td>1.0</td><td>44299.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>44300.1</td><td>44299.0</td><td>43919.925</td><td>43919.925</td><td>380.175</td><td>380.175</td><td>5.7848e17</td><td>&quot;TK2FN-FEIZAL&quot;</td><td>&quot;INSTAN&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-03</td><td>&quot;YOGYA SUBANG&quot;</td><td>&quot;GIT.2024.02.00025&quot;</td><td>&quot;OLIKE ACC&quot;</td><td>&quot;OLIKE CPH HC1 BLACK&quot;</td><td>&quot;OLIKE CAR PHONE HOLDER HC1 BLAâ€¦</td><td>&quot;YOGYA SUBANG&quot;</td><td>1.0</td><td>69520.0</td><td>8690.0</td><td>12.5</td><td>0.0</td><td>null</td><td>60830.22</td><td>60830.0</td><td>45999.998658</td><td>45999.998659</td><td>14830.221341</td><td>14830.221341</td><td>5.7848e17</td><td>&quot;CC-CHRISTIAN CHANDRA&quot;</td><td>&quot;03 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr><tr><td>null</td><td>2024-02-03</td><td>&quot;SHOPEE OL&quot;</td><td>&quot;OL.2024.02.00147&quot;</td><td>&quot;KLAR ACC&quot;</td><td>&quot;KL-DBG002&quot;</td><td>&quot;KLAR LAUNDRY DETERGENT BALLS Bâ€¦</td><td>&quot;ONLINE&quot;</td><td>1.0</td><td>11384.0</td><td>0.0</td><td>12.5</td><td>0.0</td><td>null</td><td>11384.32</td><td>11384.0</td><td>10319.448</td><td>10319.448</td><td>1064.871999</td><td>1064.872</td><td>null</td><td>&quot;SP1FN-FEIZAL&quot;</td><td>&quot;03 februari 2024 GIT ACC PROMO&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Ya&quot;</td><td>&quot;Pajak Pertambahan Nilai&quot;</td><td>null</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    }
  ]
}